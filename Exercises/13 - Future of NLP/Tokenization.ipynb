{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc25b106",
   "metadata": {},
   "source": [
    "# ðŸ§± Tokenization\n",
    "\n",
    "Welcome to this notebook on **tokenization**, the essential first step that turns raw text into something a model can actually work with. Before any language model can â€œunderstandâ€ text, the text must be broken into tokens: characters, subwords, or other meaningful units. How we choose to do that has a huge impact on model performance, vocabulary size, efficiency, and even how well the model handles rare or creative language.\n",
    "\n",
    "In this notebook, weâ€™ll explore tokenization from the ground up:\n",
    "\n",
    "- ðŸ”¤ Starting with raw bytes  \n",
    "- ðŸ§© Building up to subword methods like **Byte Pair Encoding (BPE)**  \n",
    "- âš ï¸ Examining common pitfalls and weird edge cases  \n",
    "- ðŸ” Investigating how different snippets of text get tokenized in practice  \n",
    "\n",
    "Along the way, youâ€™ll experiment, compare techniques, and uncover the surprising ways models â€œseeâ€ text. By the end, youâ€™ll not only understand how modern tokenizers work, youâ€™ll be able to build and analyze your own.\n",
    "\n",
    "Letâ€™s dive in and start breaking things apart! âœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca4d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e771e2",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Byte Pair Encoding (BPE)\n",
    "\n",
    "Youâ€™ve already seen that tokenization is about breaking text into pieces a model can understand. But language is huge and unpredictable: new words, names, typos, different spellings. If we tried to create a token for *every* word, our vocabulary would be enormousâ€¦ and still fail on words we havenâ€™t seen! ðŸ˜…\n",
    "\n",
    "**Byte Pair Encoding (BPE)** offers a clever fix. Instead of learning whole words, BPE learns **subword units**, ie. frequent chunks that can be combined to form any word. This gives us:\n",
    "\n",
    "- ðŸ”¤ **Complete coverage:** Even brand-new words can be built from familiar subwords.  \n",
    "- ðŸ“¦ **Smaller vocabulary:** Less memory, faster models, cleaner representation.\n",
    "\n",
    "### ðŸ§  The big idea  \n",
    "Start with characters â†’ repeatedly merge the most common pair â†’ build bigger and bigger chunks. Over time, the tokenizer â€œdiscoversâ€ useful patterns like roots, prefixes, and common letter combos.\n",
    "\n",
    "There are different ways to implement this algorithm. In this section, we will build together, a very simple version of it just for the purpose of understanding how it works and seeing it in action.\n",
    "\n",
    "Ready? Letâ€™s start merging! ðŸ”—âœ¨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e3d2b1",
   "metadata": {},
   "source": [
    "Letâ€™s start by loading a few early scenes from Monty Python and the Holy Grail, stored in `monty_python.txt`. Then to make things easy, let's encode all characters using UTF-8 and save them in a list, representing each byte with an integer.\n",
    "\n",
    "**`TODO:`** Load `monty_python.txt` as a `str`. Then encode each character using UTF-8 and in a list save the byte representing each character as an `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"monty_python.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "tokens = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a3a08",
   "metadata": {},
   "source": [
    "**`Discussion:`** Right now we have already encoded all characters of our initial text. Is this enough? If not, please provide the reasons why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb6d24",
   "metadata": {},
   "source": [
    "\\[Your Answer\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0365e4",
   "metadata": {},
   "source": [
    "In the next step we need to be able to compute the number of times a unique pair of characters (or in our case ids), appear in the corpus. To do this, we simply need to iterate through all pairs in our corpus and count it.\n",
    "\n",
    "**`TODO:`** Complete the following function. The function receives a `list` of integers (our tokens) and counts the number of times each pair of consecutive integers appears in the list in a `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a036766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids: List[int]) -> dict:\n",
    "    \"\"\"\n",
    "    Get counts of all adjacent token pairs in the list of token IDs.\n",
    "    Args:\n",
    "        ids (List[int]): List of token IDs.\n",
    "    Returns:\n",
    "        dict: A dictionary with token pairs as keys and their counts as values.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca9c30",
   "metadata": {},
   "source": [
    "In the next stage, we have to implement the merging step of the algorithm. In this step, we have already identified the most frequent pair (`pair`) and we'd like to replace it with a new representation `idx` that does not currently exist in our vocabulary.\n",
    "\n",
    "**`TODO:`** Complete the following function. The function receives the current representation of our tokens (`ids`), the most frequent pair (`pair`) and the new token with which we'd like to replace the most frequent pair (`idx`). Using this information it updates our token representation by replacing the most frequent pair with our indicated new token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd4d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids: List[int], pair: Tuple[int, int], idx: int) -> List[int]:\n",
    "  \"\"\"\n",
    "  Merge all occurrences of the specified token pair in the list of token IDs.\n",
    "  Args:\n",
    "    ids (List[int]): List of token IDs.\n",
    "    pair (Tuple[int, int]): The token pair to merge.\n",
    "    idx (int): The new token ID to replace the pair.\n",
    "  Returns:\n",
    "    List[int]: The updated list of token IDs with the pair merged.\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3019bf",
   "metadata": {},
   "source": [
    "Now that we have all the components that we require let's put them in a loop and see if our algorithm works ðŸ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338de8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 276 # The desired final vocabulary size\n",
    "num_merges = vocab_size - 256 # The number of merges to perform (from initial 256 byte tokens)\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(ids)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  idx = 256 + i\n",
    "  print(f\"merging {pair} into a new token {idx}\")\n",
    "  ids = merge(ids, pair, idx)\n",
    "  merges[pair] = idx\n",
    "\n",
    "print()\n",
    "print(f\"Initial token count: {len(tokens)}\")\n",
    "print(f\"Final token count: {len(ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044d517",
   "metadata": {},
   "source": [
    "## âš ï¸ Common Problems in Tokenization\n",
    "\n",
    "Tokenization may *look* simpleâ€¦ but in practice, itâ€™s full of little traps and surprises. ðŸ˜¬  \n",
    "Different languages, punctuation quirks, emojis, hyphens, typos, and even whitespace can trip up a tokenizer or lead to awkward, inefficient splits.\n",
    "\n",
    "In this section, weâ€™ll explore some of the most common issues youâ€™ll face when turning raw text into tokens, including:\n",
    "\n",
    "- ðŸŒ€ **Inconsistent splitting**\n",
    "- ðŸŒ **Multilingual and Unicode challenges**  \n",
    "- ðŸ˜µ **Strange artifacts from messy text**  \n",
    "- ðŸ”— **How subword methods sometimes break words in weird places**\n",
    "\n",
    "Weâ€™ll look at examples, and think about how modern tokenizers deal with these challenges.  \n",
    "Ready to dive into the chaos? Letâ€™s debug some tokens! ðŸ› ï¸âœ¨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54715fc9",
   "metadata": {},
   "source": [
    "### âš¡ `Tiktoken`\n",
    "\n",
    "To experiment with modern tokenization, weâ€™ll use **tiktoken**, a lightweight and fast tokenizer library designed for models like GPT. Itâ€™s built for **speed**, **efficiency**, and **consistency** with real production tokenizers.\n",
    "\n",
    "With it, you can:\n",
    "\n",
    "- ðŸ”¢ Encode text into tokens (and decode back)  \n",
    "- ðŸ“ Count tokens quickly  \n",
    "- ðŸš€ Try out different tokenization schemes used by current language models  \n",
    "\n",
    "Weâ€™ll use it for the rest of this notebook to see how real LLM tokenizers behave. Let's have a look at a quick example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hey everyone! Good luck with your Project Milestone P3 submissions!\"\n",
    "\n",
    "encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = encoder.encode(text)\n",
    "\n",
    "print(text)\n",
    "print(tokens)\n",
    "print(\"Token count:\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777299a3",
   "metadata": {},
   "source": [
    "### ðŸ” Investigating Tokenization Examples\n",
    "\n",
    "In this part, youâ€™ll investigate how different pieces of text are tokenized.  \n",
    "Your goal is simply to **look closely**, notice patterns, think about *why* the tokenizer behaves the way it does, and how this could be a problem. ðŸ•µï¸â€â™‚ï¸âœ¨\n",
    "\n",
    "You can explore the examples in **any way you prefer**:\n",
    "\n",
    "- â–¶ï¸ **Run your own code** using `tiktoken`, or  \n",
    "- ðŸŒ Use the online tokenizer tool: [link](https://tiktokenizer.vercel.app)  \n",
    "\n",
    "If you choose the web app, make sure to select the **gpt2** tokenizer so that your results match ours.\n",
    "\n",
    "Take your time, experiment, and follow your curiosityâ€”there are lots of interesting surprises hiding in how text gets chopped into tokens!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59935c7",
   "metadata": {},
   "source": [
    "**`Discussion:`** Experiments with the examples below. Tokenize them and inspect how the resulting tokens are formed. Can you see any concerning / unexpected outcomes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d548d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = \"127 + 677 = 804\"\n",
    "example2 = \"1275 + 6773 = 8041\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4cbdf",
   "metadata": {},
   "source": [
    "\\[Your Answer\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7060a",
   "metadata": {},
   "source": [
    "**`Discussion:`** Experiments with the examples below. Tokenize them and inspect how the resulting tokens are formed. Can you see any concerning / unexpected outcomes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f877b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = \"Egg.\"\n",
    "example2 = \"I have an Egg.\"\n",
    "example3 = \"egg.\"\n",
    "example4 = \"EGG.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e0808",
   "metadata": {},
   "source": [
    "\\[Your Answer\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4d5d87",
   "metadata": {},
   "source": [
    "**`Discussion:`** Experiments with the examples below. Tokenize them and inspect how the resulting tokens are formed. Can you see any concerning / unexpected outcomes?\n",
    "\n",
    "- *Note:* Examples `example2` and `example3` are french and chinese translations of `example1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b51fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = \"\"\"Once when I was six years old I saw a magnificent picture in a book, called True Stories from Nature,about the primeval forest. It was a picture of a boa constrictor in the act of swallowing an animal.\"\"\"\n",
    "example2 = \"\"\"Lorsque jâ€™avais six ans jâ€™ai vu, une fois, une magnifique image, dans un livre sur la ForÃªt Vierge qui sâ€™appelait Â« Histoires VÃ©cues Â». Ã‡a reprÃ©sentait un serpent boa qui avalait un fauve.\"\"\" \n",
    "example3 = \"\"\"æˆ‘å…­å²çš„æ—¶å€™ï¼Œåœ¨ä¸€æœ¬å«åšã€Šæ¥è‡ªå¤§è‡ªç„¶çš„çœŸå®žæ•…äº‹ã€‹çš„ä¹¦é‡Œï¼Œçœ‹åˆ°ä¸€å¼ éžå¸¸å£®è§‚çš„å›¾ç‰‡ï¼Œä¹¦é‡Œè®²çš„æ˜¯åŽŸå§‹æ£®æž—ã€‚å›¾ç‰‡ä¸Šæ˜¯ä¸€æ¡èŸ’è›‡æ­£åœ¨åžé£Ÿä¸€åªåŠ¨ç‰©ã€‚\"\"\"\n",
    "\n",
    "for example in [example1, example2, example3]:\n",
    "    print(f\"\\nExample: {example}\")\n",
    "    tokens = encoder.encode(example)\n",
    "    print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ca71a",
   "metadata": {},
   "source": [
    "\\[Your Answer\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c67fc7",
   "metadata": {},
   "source": [
    "**`Discussion:`** Experiments with the examples below. Tokenize them and inspect how the resulting tokens are formed. Can you see any concerning / unexpected outcomes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41093d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = \"\"\"\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "\"\"\"\n",
    "\n",
    "tokens = encoder.encode(example1)\n",
    "for token in tokens:\n",
    "    print(f\"'{encoder.decode([token])}' -> {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48cafd",
   "metadata": {},
   "source": [
    "\\[Your Answer\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cf0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = \"\"\"\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "\"\"\"\n",
    "\n",
    "encoder_gpt4 = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokens = encoder_gpt4.encode(example1)\n",
    "for token in tokens:\n",
    "    print(f\"'{encoder_gpt4.decode([token])}' -> {token}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
