{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "24f6a6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding, DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78866f4d",
   "metadata": {},
   "source": [
    "# Task-A: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7477fdfa",
   "metadata": {},
   "source": [
    "## A.1 Pretrained model\n",
    "In this subsection we will load a pretrained model and evaluate its performance on a movie, sentiment analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f7790",
   "metadata": {},
   "source": [
    "In Hugging Face’s Transformers library, the `Auto Classes` are generic wrappers that automatically pick the right model/tokenizer/config class for you, based on the pretrained model you’re loading.\n",
    "\n",
    "### AutoTokenizer\n",
    "- `AutoTokenizer` is a Hugging Face class that automatically picks the right tokenizer for the model you specify.\n",
    "- A tokenizer is responsible for splitting text into tokens (subwords or word pieces) that the model can understand.\n",
    "- `AutoTokenizer.from_pretrained(model_name)` downloads and loads the pretrained tokenizer for the model of your choice.\n",
    "\n",
    "### AutoModelForSequenceClassification\n",
    "- `AutoModelForSequenceClassification` is a Hugging Face class that loads a model configured for sequence classification (e.g., sentiment analysis, text classification).\n",
    "- `AutoModelForSequenceClassification.from_pretrained(model_name)`This method loads a model that has already been trained (pretrained) and published.\n",
    "\n",
    "This is the setup we’re using for the following exercise. In practice, Hugging Face provides many additional classes for a wide variety of setups. You are encouraged to explore the [Auto Classes documentation](https://huggingface.co/docs/transformers/en/model_doc/auto) to get a broader understanding of what’s available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4fc6fc",
   "metadata": {},
   "source": [
    "**`TODO:`** Load the tokenizer and the pretrained model for sequence classification for `distilbert/distilbert-base-uncased`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4173b5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0ca88528e241758d64c0587b763306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02a38346e9943768b54f52b086765f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48ffcca90874673a1930b50cbea36b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a30523ce6f4beeac45d8d2de171eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdfec7c0cb2844bfa29fabf41ddf4be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert/distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2839f",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "- The datasets library is another core library from Hugging Face, separate from transformers.\n",
    "- In short, this library is designed to make it easy to access, share, preprocess, and work with large datasets (especially for NLP, but also vision, audio, and multimodal tasks).\n",
    "- The `load_dataset(\"dataset_name\")` function pulls and prepares a dataset from the internet.\n",
    "- The `load_from_disk(\"dataset_name\")` function loads a local dataset in the same way.\n",
    "- The `.map()` method applies a function to every element (or batch of elements) in a Dataset. Have a look at an example here: [link](https://huggingface.co/docs/datasets/en/process#map).\n",
    "- When loading the dataset using the above function, the `split` argument can be used to get a specific split.\n",
    "- Note: These are different from the PyTorch datasets. They're similar and it's easy to transition from one to the other but they're not identical.\n",
    "\n",
    "For further documentation regarding HF dataset, you are encouraged to explore the following [documentation](https://huggingface.co/docs/datasets/en/index)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b520005",
   "metadata": {},
   "source": [
    "**`TODO:`** Load the train and test split of the `imdb` dataset. How many samples are in each split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd452673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Training samples:  25000\n",
      "#Test samples:  25000\n"
     ]
    }
   ],
   "source": [
    "train_data = load_dataset(\"imdb\", split=\"train\")\n",
    "test_data = load_dataset(\"imdb\", split=\"test\")\n",
    "\n",
    "print('#Training samples: ', len(train_data))\n",
    "print('#Test samples: ', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f56d8d",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "- `DataLoader` is a PyTorch utility that wraps a dataset and handles batching, shuffling, and parallel loading.\n",
    "- It takes a dataset (can be from both HF and PyTorch) and returns an iterator you can loop over in training or evaluation.\n",
    "\n",
    "Have a look at its [documentation](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). Specifically, see what arguments are need to load a dataset, to set your own batch size and decide whether the data will be shuffled or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e42df",
   "metadata": {},
   "source": [
    "**`TODO:`** Load your test data into a `DataLoader` of batch size 16. Do not shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d70e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d928d447",
   "metadata": {},
   "source": [
    "**`TODO:`** As we have previously mentioned, `DataLoader` returns an iterator. Using a `for` loop, investigate what the iterator returns for its first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a82fda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.', \"Worth the entertainment value of a rental, especially if you like action movies. This one features the usual car chases, fights with the great Van Damme kick style, shooting battles with the 40 shell load shotgun, and even terrorist style bombs. All of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before.<br /><br />The plot is made interesting by the inclusion of a rabbit, which is clever but hardly profound. Many of the characters are heavily stereotyped -- the angry veterans, the terrified illegal aliens, the crooked cops, the indifferent feds, the bitchy tough lady station head, the crooked politician, the fat federale who looks like he was typecast as the Mexican in a Hollywood movie from the 1940s. All passably acted but again nothing special.<br /><br />I thought the main villains were pretty well done and fairly well acted. By the end of the movie you certainly knew who the good guys were and weren't. There was an emotional lift as the really bad ones got their just deserts. Very simplistic, but then you weren't expecting Hamlet, right? The only thing I found really annoying was the constant cuts to VDs daughter during the last fight scene.<br /><br />Not bad. Not good. Passable 4.\"]\n",
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Check what is in the data loader?\n",
    "for batch in test_loader:\n",
    "  print(batch['text'])\n",
    "  print(batch['label'])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bf941c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.']\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, tokenizer, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate a Hugging Face sequence classification model on a test dataset.\n",
    "\n",
    "    Parameters:\n",
    "        model (transformers.AutoModelForSequenceClassification): The pretrained sequence classification model to evaluate.\n",
    "        tokenizer (transformers.AutoTokenizer): The tokenizer used to preprocess the input text for the model.\n",
    "        test_loader (torch.utils.data.DataLoader): A DataLoader providing the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        all_preds (torch.Tensor): Model predictions on the test set.\n",
    "        all_labels (torch.Tensor): Ground truth labels from the test set.\n",
    "        acc (float): Overall accuracy on the test set.\n",
    "        f1 (float): F1 score on the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    all_labels = None\n",
    "    all_preds = None\n",
    "\n",
    "    for batch in tqdm(test_loader):\n",
    "        text = batch['text']\n",
    "        labels = batch['label']\n",
    "\n",
    "        # TODO: Tokenize the text using the provided tokenizer, use both truncation and padding.\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.to('cuda')\n",
    "            model = model.to('cuda')\n",
    "\n",
    "        # TODO: Perform a forward pass through the output of the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # TODO: Get the logits from the model's output and compute the predictions by taking the argmax\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1).cpu()\n",
    "\n",
    "        if all_labels is None:\n",
    "            all_labels = labels.cpu()\n",
    "            all_preds = preds.cpu()\n",
    "        else:\n",
    "            all_labels = torch.cat((all_labels, labels.cpu()))\n",
    "            all_preds = torch.cat((all_preds, preds.cpu()))\n",
    "\n",
    "    # TODO: compute f1 score between model predictions and ground-truth labels (you can use sklearn.metrics)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    # TODO: compute accuracy score between model predictions and ground-truth labels (you can use sklearn.metrics)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    # TODO: compute the accuracy on Positive(label==1) samples\n",
    "    pos_acc = accuracy_score(all_labels[all_labels==1], all_preds[all_labels==1])\n",
    "\n",
    "    # TODO: compute the accuracy on Negative(label==0) samples\n",
    "    neg_acc = accuracy_score(all_labels[all_labels==0], all_preds[all_labels==0])\n",
    "\n",
    "    print('Accuracy: ', acc*100, '%')\n",
    "    print(' -- Positive Accuracy: ', pos_acc*100, '%')\n",
    "    print(' -- Negative Accuracy: ', neg_acc*100, '%')\n",
    "    print('F1 score: ', f1)\n",
    "\n",
    "    return all_preds, all_labels, acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7acb1",
   "metadata": {},
   "source": [
    "## A.2 Finetuned model\n",
    "In this section, we will further train the model for the task that we are interested in and see if we can increase its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94071258",
   "metadata": {},
   "source": [
    "**`TODO:`** Use `evaluate_model` to measure the performance of the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e03edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_labels, acc, f1 = evaluate_model(model, tokenizer, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f260f",
   "metadata": {},
   "source": [
    "**`TODO:`** Define a function that receives some samples and then uses the tokenizer we have defined to tokenize the samples. Use the `Dataset.map`method that we have previously discussed to apply your function to the train data. Keep this in mind when defining the tokenizing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94c6e0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(samples):\n",
    "    return tokenizer(samples['text'], truncation=True)\n",
    "\n",
    "# TODO: Tokenize the training data\n",
    "tokenized_train = train_data.map(tokenize, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b876d3",
   "metadata": {},
   "source": [
    "### Data Collator\n",
    "- A data collator is a small function/class that tells the DataLoader how to merge a list of individual samples into a single batch.\n",
    "- In NLP, a main challenge is padding. Since sentences have different lengths, you need to pad them so they fit into a uniform tensor batch.\n",
    "- The `DataCollatorWithPadding` will  automatically pad sequences in a batch to the length of the longest sequence in that batch (dynamic padding) based on the tokenizer you're using.\n",
    "\n",
    "For more info on Data Collators please refer to the following [documentation](https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorWithPadding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18043488",
   "metadata": {},
   "source": [
    "**`TODO:`** Define a data collator that automatically pads sequences in a batch based on the defined `tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661faa3a",
   "metadata": {},
   "source": [
    "### Training Arguments\n",
    "- A configuration object that stores all the knobs and settings related to the training procedure (like learning rate, batch size, number of epochs, output directory, etc.).\n",
    "- It tells the training loop how to train (e.g., optimization settings, saving/checkpoint rules, logging options).\n",
    "- You don’t train with it directly, you just define the \"rules of training.\"\n",
    "\n",
    "### Trainer\n",
    "- This is the high-level training loop: it actually runs the training, evaluation, and prediction based on your model, data, and `TrainingArguments`.\n",
    "- It takes care of all the heavy lifting (forward pass, loss calculation, backprop, optimizer steps, checkpoint saving, etc.).\n",
    "- You just call methods like `.train()`, `.evaluate()`, or `.predict()` without writing a manual loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75570a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_name = \"imdb-finetuned-distilbert\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir_name,          # Where to save model checkpoints and logs\n",
    "    learning_rate=2e-5,                  # Step size for the optimizer (how fast the model learns)\n",
    "    per_device_train_batch_size=16,      # Training batch size per GPU/CPU device\n",
    "    per_device_eval_batch_size=16,       # Evaluation batch size per GPU/CPU device\n",
    "    num_train_epochs=1,                  # Number of times to iterate over the full training dataset\n",
    "    weight_decay=0.01,                   # Strength of L2 regularization (helps prevent overfitting)\n",
    "    save_strategy=\"epoch\",               # When to save checkpoints (\"epoch\" = at the end of each epoch)\n",
    "    push_to_hub=False,                   # Whether to push the model to the Hugging Face Hub\n",
    "    report_to=\"none\"                     # Where to report logs (e.g., \"wandb\", \"tensorboard\", \"none\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773061ac",
   "metadata": {},
   "source": [
    "**`TODO:`** Based on everything what we have defined so far in this exercise, complete the following code to initialize the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize the trainer\n",
    "trainer = Trainer(\n",
    "   model = model,\n",
    "   args = training_args,\n",
    "   train_dataset = tokenized_train,\n",
    "   tokenizer = tokenizer,\n",
    "   data_collator = data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6909f",
   "metadata": {},
   "source": [
    "**`TODO:`** Use the `.train` method of the trainer to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ad3eb4",
   "metadata": {},
   "source": [
    "**`TODO:`** Use `evaluate_model` to measure the performance of the post-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9448c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_labels, acc, f1 = evaluate_model(model, tokenizer, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e5130",
   "metadata": {},
   "source": [
    "# Task-B: Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba497e",
   "metadata": {},
   "source": [
    "### `text_target` in Tokenizers\n",
    "\n",
    "- The text_target argument is used when working with sequence-to-sequence (encoder–decoder) models such as T5, BART, mBART, or mT5.\n",
    "- It allows you to tokenize the target/output text (e.g. a translation or summary) alongside the input text in a single call to the tokenizer.\n",
    "- The tokenized targets are stored under the key labels, which the model uses during training for loss computation.\n",
    "\n",
    "### AutoModelForSeq2SeqLM\n",
    "- `AutoModelForSeq2SeqLM` is a Hugging Face class that loads a model configured for sequence-to-sequence tasks (e.g., machine translation, text summarization, question answering, text generation with input-output pairs).\n",
    "- These models typically take in a sequence as input (e.g., a sentence or paragraph) and generate a new sequence as output (e.g., a translated or summarized version).\n",
    "\n",
    "Note: The specific tokenizer will require you to `pip install protobuf`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55145b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/mt5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b59b0",
   "metadata": {},
   "source": [
    "**`TODO:`** Use the tokenizer to tokenize both an input and a target in one go. You can use \"Hello\" for input and \"Bonjour\" for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5928716a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [30273, 1], 'attention_mask': [1, 1], 'labels': [259, 66392, 1]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = tokenizer(\"Hello\", text_target=\"Bonjour\")\n",
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b624362",
   "metadata": {},
   "source": [
    "**`TODO:`** Unzip the `wmt14_fr_en_10k` dataset and load it into a HF `Dataset`. Split the data into a training and a validation set. To minimize the procedure, use the `.select(N)` method of the `Dataset` to use only 3000 samples for training and 50 for validation. Then, print the data, to see their feaures and ensure the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83c9f56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data = load_from_disk('wmt14_fr_en_10k')\n",
    "data[\"train\"] = data[\"train\"].select(range(3000))\n",
    "data[\"validation\"] = data[\"validation\"].select(range(50))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac6d27",
   "metadata": {},
   "source": [
    "**`TODO:`** Define a function that receives some samples and then uses the tokenizer we have defined to tokenize the samples. Append the phrase `\"translate English to French: \"` to the inputs. Tokenize the targets (french translation) as well. Use the `Dataset.map`method that we have previously discussed to apply your function to all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df33ebc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77467ff2ab947dc9b79ca30cd8b0e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d977357fadbe4284abda0b745358a847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e281ffb1bab4424d85b38a6bae8ac7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    prefix = \"translate English to French: \"\n",
    "    inputs = [prefix + src[\"en\"] for src in examples[\"translation\"]]\n",
    "    targets = [tgt[\"fr\"] for tgt in examples[\"translation\"]]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=64, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2652e7",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "- Hugging Face's library for evaluations.\n",
    "- By importing `evaluate` the library provides ready-to-use implementations of common metrics (accuracy, F1, BLEU, ROUGE, etc.).\n",
    "- The `evaluate.load(\"sacrebleu\")` method, loads the SacreBLEU metric, a standard metric for evaluating machine translation quality. Give it a try in the following example. Any ideas about what the results could mean?\n",
    "\n",
    "More information regarding HF Evaluate can be found in the following [documentation](https://huggingface.co/docs/evaluate/en/index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b68abbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 29.05925408079185, 'counts': [5, 2, 1, 0], 'totals': [6, 5, 4, 3], 'precisions': [83.33333333333333, 40.0, 25.0, 16.666666666666668], 'bp': 0.846481724890614, 'sys_len': 6, 'ref_len': 7}\n"
     ]
    }
   ],
   "source": [
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "predictions = [\"the cat is on the mat\"]\n",
    "references = [[\"there is a cat on the mat\"]]\n",
    "\n",
    "results = sacrebleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d76432b",
   "metadata": {},
   "source": [
    "### Data COllators (continued)\n",
    "- The DataCollatorForSeq2Seq is a special collator for sequence-to-sequence tasks (like translation or summarization). It not only handles dynamic padding, but also makes sure that both the inputs and the labels (decoder side) are correctly padded. It can also prepare the labels for the model’s loss function (e.g. replacing padding tokens with -100 so they’re ignored during loss computation).\n",
    "- Because of this, the model as well as the tokenizer are required to initialize it: it uses the model’s configuration (e.g. label_pad_token_id, eos_token_id) to ensure that the labels it produces match exactly what the model expects for training and loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "95805e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac159a76",
   "metadata": {},
   "source": [
    "Same as before, we need to be able to evaluate our model. Below is the evaluation method that we have chosen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
