{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf74937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from peft import LoraConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d30b09",
   "metadata": {},
   "source": [
    "In this notebook we will continue working with the GSM8K dataset. So, let's dig in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770757e5",
   "metadata": {},
   "source": [
    "**`TODO:`** Load the `\"openai/gsm8k\"` dataset directly (don't split it into train and test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cacb562",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"openai/gsm8k\", \"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca0893f",
   "metadata": {},
   "source": [
    "## The `messages` Format for Chat Data\n",
    "\n",
    "When training modern large language models (LLMs) for **instruction following** or **chat**, data is usually represented in a structured format called `messages`.\n",
    "\n",
    "### What is `messages`?\n",
    "\n",
    "- `messages` is a **list of objects**.\n",
    "- Each object represents **one turn** in a conversation.\n",
    "- Every object has at least two fields:\n",
    "  - `\"role\"` → who is speaking (`\"user\"`, `\"assistant\"`, or `\"system\"`)\n",
    "  - `\"content\"` → the text spoken by that role\n",
    "\n",
    "### Example\n",
    "\n",
    "```json\n",
    "\"messages\": [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"},\n",
    "  {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"The answer is 4.\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba973c85",
   "metadata": {},
   "source": [
    "**`TODO:`**  \n",
    "Write a function that reformats each sample by creating a new `messages` field.  \n",
    "\n",
    "- Each sample should contain a list of dictionaries, where each dictionary has a `\"role\"` and a `\"content\"` key.  \n",
    "- The `question` becomes a message with `\"role\": \"user\"`.  \n",
    "- The `answer` becomes a message with `\"role\": \"assistant\"`.  \n",
    "- A `\"system\"` message is not required and should be omitted.  \n",
    "\n",
    "After defining the function, apply it to the dataset using `.map()`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e36c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_gsm8k(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": f\"{sample['question']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "data_fmt = data.map(format_gsm8k)\n",
    "\n",
    "assert \"messages\" in data_fmt[\"train\"].features, \"The 'messages' feature is missing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a2f77",
   "metadata": {},
   "source": [
    "### Qwen2-0.5B\n",
    "- **Qwen2-0.5B** is part of the Qwen2 model family, released by Alibaba as a compact dense language model.  \n",
    "- It is a **decoder-only** model designed for text generation and general language tasks.  \n",
    "- An **instruction-tuned variant** (“Instruct”) is also available, optimized to follow prompts more reliably.  \n",
    "- Qwen2-0.5B is useful for tasks like chat, text completion, and prompt-based generation, especially when a smaller yet capable model is preferred.  \n",
    "\n",
    "You can check out the model card [here](https://huggingface.co/Qwen/Qwen2-0.5B).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5fa6ff",
   "metadata": {},
   "source": [
    "**`TODO:`**  Load the tokenizer for `Qwen/Qwen2-0.5B`, and if it does not have a `pad_token`, set it equal to the `eos_token`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eef2577",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2-0.5B\"  # or whichever base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89994865",
   "metadata": {},
   "source": [
    "**`TODO`**: Tokenize the `messages` feature of the dataset by defining a function that does this and then using `.map`. Keep in mind that for this to be done you have to flatten the `dict` object but still keep the roles in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example):\n",
    "    # Convert the messages list into a simple string format\n",
    "    full = \"\"\n",
    "    for msg in example[\"messages\"]:\n",
    "        prefix = \"User: \" if msg[\"role\"] == \"user\" else \"Assistant: \"\n",
    "        full += prefix + msg[\"content\"] + \"\\n\"\n",
    "    return tokenizer(\n",
    "        full,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "tokenized = data_fmt.map(tokenize_fn, batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49f493",
   "metadata": {},
   "source": [
    "## SFTTrainer\n",
    "- The `trl` library (short for **Transformers Reinforcement Learning**) extends Hugging Face’s ecosystem with tools for fine-tuning and alignment.  \n",
    "- **`SFTTrainer`** is a class in this library designed for **Supervised Fine-Tuning (SFT)** of language models.  \n",
    "- It simplifies training by wrapping the Hugging Face `Trainer` with defaults tailored for instruction tuning and chat-like datasets.  \n",
    "- You define your training setup with an `SFTConfig`, which specifies parameters like batch size, learning rate, number of epochs, evaluation steps, and more.  \n",
    "- The `SFTTrainer` takes the model, your tokenized dataset, and the config, and runs the fine-tuning loop automatically.  \n",
    "- By default, it can also handle chat-style formatting (with roles like *user* and *assistant*), which is useful when fine-tuning dialogue models.  \n",
    "\n",
    "For further documentation regarding `SFTTrainer`, you are encouraged to explore the following [documentation](https://huggingface.co/docs/trl/en/sft_trainer).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6136896",
   "metadata": {},
   "source": [
    "### SFTConfig\n",
    "- **`SFTConfig`** is a configuration class used for **Supervised Fine-Tuning (SFT)** of language models.  \n",
    "- It stores all the key training settings in one place (like batch size, learning rate, epochs, logging, saving, etc.).  \n",
    "- This config is passed to the trainer to control *how* the fine-tuning process runs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e7d87e",
   "metadata": {},
   "source": [
    "**`TODO:`** Review the hyperparameters below and ensure you understand each; adjust values like `num_train_epochs`, `per_device_train_batch_size`, or `gradient_accumulation_steps` if training is too slow or your hardware can’t handle the defaults.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f60aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SFTConfig(\n",
    "    output_dir=\"gsm8k-instruct\",      # folder where the trained model and checkpoints will be saved\n",
    "    num_train_epochs=3,               # number of times the model will see the entire training dataset\n",
    "    per_device_train_batch_size=32,   # how many examples each GPU processes at once\n",
    "    gradient_accumulation_steps=2,    # accumulate gradients for 2 steps before updating weights (acts like a larger batch size)\n",
    "    learning_rate=2e-5,               # how fast the model's weights are updated\n",
    "    logging_steps=10,                 # log training metrics every 10 steps\n",
    "    save_strategy=\"steps\",            # choose when to save checkpoints (here: every few steps)\n",
    "    save_steps=200,                   # save a checkpoint every 200 steps\n",
    "    eval_strategy=\"steps\",            # choose when to run evaluation (here: every few steps)\n",
    "    eval_steps=200,                   # run evaluation every 200 steps\n",
    "    bf16=True,                        # use bfloat16 precision for faster training and lower memory use (if supported by GPU)\n",
    "    packing=False,                    # do not pack multiple sequences into one sample (requires special formatting if True)\n",
    "    dataset_text_field=None,          # not needed here, since we already tokenized the data\n",
    "    assistant_only_loss=True          # compute the loss only on the assistant’s outputs, not the user’s prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b50a2b",
   "metadata": {},
   "source": [
    "**`TODO:`** Using the documentation provided above, define an `SFTTrainer` with your model, config, and processed dataset, then call train the chosen model(use `.select()` to reduce the dataset size if training is too slow).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811320b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train with LoRA\n",
    "trainer = SFTTrainer(\n",
    "    model=model_name,\n",
    "    train_dataset=tokenized[\"train\"].shuffle(seed=42).select(range(3000)),\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    args=config\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"gsm8k-instruct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c940943",
   "metadata": {},
   "source": [
    "### Evaluating Our Fine-Tuned Model\n",
    "Let’s see how our model performs after Supervised Fine-Tuning!  \n",
    "We’ll reload the trained model, run it on a few test questions, and compare its answers against the ground truth.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd1dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the fine-tuned model\n",
    "pipe = pipeline(\"text-generation\", model=\"gsm8k-instruct\", device_map=\"auto\")\n",
    "\n",
    "# Grab a few samples from test set\n",
    "samples = tokenized[\"test\"].shuffle(seed=0).select(range(5))\n",
    "\n",
    "for s in samples:\n",
    "    q = s[\"question\"]\n",
    "    print(\"Q:\", q)\n",
    "    out = pipe(f\"Solve step by step:\\n{q}\", max_new_tokens=200, do_sample=False)\n",
    "    print(\"Model answer:\", out[0][\"generated_text\"])\n",
    "    print(\"Ground truth:\", s[\"answer\"])\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd7221b",
   "metadata": {},
   "source": [
    "## PEFT\n",
    "- PEFT (Parameter-Efficient Fine-Tuning) is a collection of methods for adapting large pre-trained models without updating all of their parameters.  \n",
    "- Instead of retraining the entire model, PEFT techniques introduce or adjust a small set of additional parameters (e.g., adapters, prompts, low-rank matrices) while keeping most of the original model frozen.  \n",
    "- This approach greatly reduces the computational cost, memory usage, and storage requirements of fine-tuning, making it practical to personalize and deploy large language models on smaller hardware.  \n",
    "- Widely used in research and production for customizing large models to specific tasks, domains, or user data in a cost-effective way.  \n",
    "\n",
    "You can explore Hugging Face’s [`peft`](https://huggingface.co/docs/peft/index) library, which implements popular PEFT methods like LoRA, prefix tuning, and prompt tuning.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268bbbf",
   "metadata": {},
   "source": [
    "### LoRA (LoraConfig)\n",
    "- LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning (PEFT) method that adapts large pre-trained models by injecting low-rank trainable matrices into specific layers (commonly the attention layers).  \n",
    "- Instead of updating the full weight matrices, LoRA represents weight updates as the product of two smaller matrices, drastically reducing the number of trainable parameters.  \n",
    "- This makes fine-tuning more memory-efficient, faster to train, and easier to deploy, while allowing multiple LoRA modules to be composed for different tasks.  \n",
    "- Widely adopted in NLP and generative AI for customizing large language models without the heavy costs of full fine-tuning.  \n",
    "\n",
    "In the Hugging Face `peft` library, LoRA is configured using [`LoraConfig`](https://huggingface.co/docs/peft/package_reference/lora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_cfg = LoraConfig(\n",
    "    r=16,  # (int) Rank of the low-rank update matrices. \n",
    "           # Controls the adaptation capacity. Typical values: 4–64. \n",
    "           # Higher = more expressive but more parameters.\n",
    "\n",
    "    lora_alpha=32,  # (int) Scaling factor for the LoRA updates. \n",
    "                    # The effective weight update is scaled by alpha/r. \n",
    "                    # Larger values give stronger adaptation.\n",
    "\n",
    "    lora_dropout=0.05,  # (float) Dropout probability applied to LoRA layers during training. \n",
    "                        # Helps regularize and prevent overfitting. \n",
    "                        # Set to 0.0 to disable.\n",
    "\n",
    "    task_type=\"CAUSAL_LM\"  # (str) Type of task for which LoRA is applied. \n",
    "                           # Must match the model family. \n",
    "                           # Common values:\n",
    "                           #   \"CAUSAL_LM\"   → decoder-only LMs (e.g., GPT-style)\n",
    "                           #   \"SEQ_2_SEQ_LM\" → encoder-decoder LMs (e.g., T5, BART)\n",
    "                           #   \"TOKEN_CLS\"    → token classification\n",
    "                           #   \"SEQ_CLS\"      → sequence classification\n",
    "                           #   \"QUESTION_ANS\" → QA tasks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6512ea",
   "metadata": {},
   "source": [
    "**`TODO:`** Retrain the model using **LoRA**. Compared to standard SFT, only two adjustments are needed:  \n",
    "- Update your `SFTConfig` if you want to save the model in a new folder (and optionally raise the learning rate).  \n",
    "- Pass a `LoraConfig` object to the trainer via the `peft_config` argument (check the documentation if unsure).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update training config for LoRA fine-tuning -> No big changes\n",
    "config = SFTConfig(\n",
    "    output_dir=\"gsm8k-instruct-lora\", # Change 1: Just to save in a different folder\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4,               # Change 2: Usually this is higher than full fine-tuning\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    bf16=True,\n",
    "    packing=False,\n",
    "    assistant_only_loss=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model_name,                   # base model (string or loaded)\n",
    "    train_dataset=tokenized[\"train\"].shuffle(seed=42).select(range(3000)),\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    args=config,\n",
    "    peft_config=lora_cfg                # ← enables LoRA\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"gsm8k-instruct-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf910c39",
   "metadata": {},
   "source": [
    "## DPO\n",
    "- DPO (Direct Preference Optimization) is a training method for aligning language models with human preferences without the need for reinforcement learning.  \n",
    "- Instead of using a reward model + reinforcement learning (like in RLHF), DPO directly optimizes the model on preference data (pairs of “chosen” vs “rejected” responses).  \n",
    "- This makes alignment training simpler, more stable, and often more efficient while still steering models towards producing responses preferred by humans.  \n",
    "- Widely used in instruction-tuning and alignment pipelines as a lightweight alternative to RLHF.  \n",
    "\n",
    "You can explore Hugging Face’s [`DPO`](https://huggingface.co/docs/trl/main/en/dpo_trainer) documentation in the `trl` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f96abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_cfg = DPOConfig(\n",
    "    beta=0.1,  # (float) Inverse temperature for preference optimization. \n",
    "               # Controls sharpness of the preference signal. \n",
    "               # Lower beta → stronger emphasis on chosen vs rejected. \n",
    "               # Typical range: 0.05 – 0.2.\n",
    "\n",
    "    loss_type=\"sigmoid\",  # (str) Loss function for comparing chosen vs rejected outputs.\n",
    "                          # Options:\n",
    "                          #   \"sigmoid\" (default) → smooth logistic preference loss\n",
    "                          #   \"hinge\" → margin-based loss\n",
    "\n",
    "    label_smoothing=0.0,  # (float) Applies label smoothing to the preference loss. \n",
    "                          # Helps regularize training when preference data is noisy. \n",
    "                          # Common values: 0.0 (none) – 0.1.\n",
    "\n",
    "    max_length=512,       # (int) Maximum total sequence length (prompt + response).\n",
    "                          # Longer sequences will be truncated.\n",
    "\n",
    "    max_prompt_length=128,# (int) Maximum length for the prompt portion only. \n",
    "                          # Ensures balanced context vs response length.\n",
    "\n",
    "    max_target_length=384,# (int) Maximum length for the generated response. \n",
    "                          # Ensures responses don’t dominate input length.\n",
    "\n",
    "    truncation_mode=\"keep_end\",  \n",
    "                          # (str) How to truncate when sequences exceed max length.\n",
    "                          #   \"keep_end\" → keep last tokens (default, useful for responses)\n",
    "                          #   \"keep_start\" → keep first tokens (useful for prompts)\n",
    "\n",
    "    generate_during_eval=False, \n",
    "                          # (bool) If True, generates model outputs during evaluation \n",
    "                          # instead of just scoring given responses. \n",
    "                          # More realistic but slower.\n",
    "\n",
    "    # Standard Trainer config options (like learning_rate, batch sizes, logging, etc.) \n",
    "    # can also be passed when initializing the trainer.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e27299",
   "metadata": {},
   "source": [
    "### DPO Datasets\n",
    "- DPO (Direct Preference Optimization) requires datasets where each example contains a **prompt** and two possible responses:  \n",
    "  - **chosen** → the response preferred by humans (or a proxy, e.g. a reward model).  \n",
    "  - **rejected** → the less-preferred response.  \n",
    "- During training, the model is optimized to assign higher probability to the **chosen** response compared to the **rejected** one, given the same prompt.  \n",
    "- This format is much simpler than reinforcement learning with a reward model, since the training objective is directly defined over these pairs.  \n",
    "- Widely used in alignment pipelines for instruction-tuned LLMs, where the dataset may come from human annotations, pairwise preference collection, or synthetic generation (e.g., using a reward model or rule-based scoring).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56232f7e",
   "metadata": {},
   "source": [
    "**`TODO:`** Load and explore the `xinlai/Math-Step-DPO-10K`dataset. What are the features? Print a few samples to get a good idea on what you're working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e0649",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"xinlai/Math-Step-DPO-10K\")\n",
    "print(dataset + \"\\n\")\n",
    "\n",
    "for k, v in dataset[\"train\"][0].items():\n",
    "    print(f\"{k}: {v}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8047efb",
   "metadata": {},
   "source": [
    "**`TODO:`** Load your trained model and further train it using DPO on the following last given dataset. This time you need to train a `DPOTrainer` rather than a `SFTTrainer` but everything else remains relatively similar. For examples on how to do this, have a look at the documentation provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b3f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gsm8k-instruct-lora\"  # or your SFT checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model_name,\n",
    "    ref_model=model_name,   # frozen reference (usually SFT model)\n",
    "    args=config,\n",
    "    beta=0.1,               # controls preference strength\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"gsm8k-instruct-dpo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
