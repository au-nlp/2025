{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c4afa4",
   "metadata": {},
   "source": [
    "# Prompting\n",
    "\n",
    "Welcome! In this notebook, you‚Äôll explore how large language models (LLMs) can be guided to solve **mathematical reasoning problems** using the ‚ö° **Groq API**.  \n",
    "We‚Äôll work with the üßÆ **GSM8k dataset**, a benchmark designed to test step-by-step math reasoning.  \n",
    "\n",
    "### üîç What you‚Äôll do\n",
    "- üîë **Set up Groq** ‚Üí get your API key and connect to the Groq service.  \n",
    "- üìÇ **Load the GSM8k dataset** ‚Üí explore the structure of questions, answers, and reasoning.  \n",
    "- ‚úçÔ∏è **Experiment with prompting** ‚Üí try different styles of instructions and demonstrations.  \n",
    "- üõ†Ô∏è **Build and test solvers** ‚Üí send structured prompts to the model and parse responses.  \n",
    "- üéõÔ∏è **Play and improve** ‚Üí adjust few-shot examples, prompts, models, and generation parameters to push performance further.  \n",
    "\n",
    "‚ú® By the end, you‚Äôll see how **prompt design + reasoning demonstrations + model settings** can dramatically change performance on challenging reasoning tasks. Have fun experimenting!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa63584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import regex\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ea2af",
   "metadata": {},
   "source": [
    "### Groq\n",
    "- Groq builds specialized processors (LPUs: Language Processing Units) designed to run large AI models extremely fast and efficiently compared to traditional GPUs/CPUs.\n",
    "- Their chips and software stack focus on minimizing inference latency, making them well-suited for real-time AI applications like chatbots, speech, and recommendation systems.\n",
    "- Groq provides APIs that let you run models (like LLMs) on their hardware in the cloud, so you can use their speed without owning the hardware yourself.\n",
    "\n",
    "Check out Groq's website: [link](https://groq.com/)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888d4c8",
   "metadata": {},
   "source": [
    "**`TODO:`** \n",
    "\n",
    "1. Create a personal account on **Groq**.  \n",
    "   - For this exercise, you can remain on the **Free tier**‚Äîno payment details or credit card are required.  \n",
    "\n",
    "2. Generate an **API key** and store it in your environment variables under the name `GROQ_API_KEY`.  \n",
    "\n",
    "3. Verify your setup by running the following code snippet.  \n",
    "   - Once confirmed, feel free to experiment with different prompts and models.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f39bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c618487",
   "metadata": {},
   "source": [
    "### GSM8k\n",
    "- GSM8K (Grade School Math 8K) is a benchmark dataset designed to evaluate the mathematical reasoning ability of language models.\n",
    "- It contains around 8,500 high-quality, linguistically diverse word problems that require multi-step reasoning at the grade school (middle-school level) math level.\n",
    "- Widely used in research for training and evaluating large language models, especially in testing their ability to perform step-by-step reasoning rather than just recall facts.\n",
    "\n",
    "You can have a look at the Dataset's card [here](https://huggingface.co/datasets/openai/gsm8k)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d61d36",
   "metadata": {},
   "source": [
    "**`TODO:`**  \n",
    "\n",
    "- Load the **train** and **test** splits of the `gsm8k` dataset using Hugging Face.  \n",
    "- This dataset has two configurations: `main` and `socratic`.  \n",
    "  - Be sure to set `name=\"main\"` when loading to select the main configuration.  \n",
    "- üí° Hint: If you need a refresher on loading a `Dataset` from Hugging Face, revisit the **Lab Session 7** exercises on fine-tuning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1a10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "dataset_test = load_dataset(\"gsm8k\", \"main\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b6cbf",
   "metadata": {},
   "source": [
    "**`TODO:`** Check out the features and the number of rows in the train set. For the first two sample, print all of its features to get an understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset_train)\n",
    "\n",
    "sample = dataset_train[0]\n",
    "print(f\"Question:\\n{sample['question']}\\n\")\n",
    "print(f\"Answer:\\n{sample['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff2ada",
   "metadata": {},
   "source": [
    "### Introduciton to Reasoning\n",
    "\n",
    "As you might have noticed the answers in the dataset can be divided in two parts (1) the actual, final response and (2) the reasoning needed to get there. ‚ÄúReasoning‚Äù is the step-by-step explanation from the dataset that shows how the final answer was derived before the `####` marker. These steps are often called thoughts as well. Thoughts or steps are the individual actions, each one an independent move in the solver‚Äôs attempt to work through the problem and reach the final answer. Reasoning can be segmented in different ways depending on the dataset or the solver‚Äôs style, but for our exercise we‚Äôll keep it simple and treat each sentence in the reasoning as a single thought."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d3e6f",
   "metadata": {},
   "source": [
    "**`TODO:`** Create a few simple answer demonstrations.  \n",
    "\n",
    "Complete the code below so that it generates `qa_obj` dictionaries containing **only**:\n",
    "- the `question`\n",
    "- the *final response*\n",
    "\n",
    "Make sure the reasoning steps are excluded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_demonstrations = []\n",
    "for i, row in enumerate(dataset_train.shuffle(seed=42)):\n",
    "    qa_obj = {}\n",
    "    # TODO: only include the question and the final answer (no reasoning steps)\n",
    "    qa_obj['Question'] = row['question']\n",
    "    qa_obj['Response'] = row['answer'].split('####')[-1].strip()\n",
    "    answer_demonstrations.append(qa_obj)\n",
    "    if i==20:\n",
    "        break\n",
    "answer_demonstrations[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b87bf30",
   "metadata": {},
   "source": [
    "**`TODO:`** Now let's create some demonstrations that include reasoning as well.\n",
    "- For each sample, create a new field called `\"Response\"`.  \n",
    "  - `\"Response\"` should be a dictionary with the following keys:  \n",
    "    - `\"Thoughts\"` ‚Üí a list capturing the reasoning steps or intermediate thoughts leading to the solution.  \n",
    "    - `\"Answer\"` ‚Üí the final numeric answer to the problem.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c044fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_demonstrations = []\n",
    "for i, row in enumerate(dataset_train.shuffle(seed=42)):\n",
    "    qa_obj = {}\n",
    "    qa_obj['Question'] = row['question']\n",
    "    qa_obj['Response'] = {}\n",
    "    # TODO: Split the answer into thoughts and final answer\n",
    "    qa_obj['Response']['Thoughts'] = row['answer'].split('####')[0].strip().split('\\n')\n",
    "    qa_obj['Response']['Answer'] = row['answer'].split('####')[-1].strip()\n",
    "    reason_demonstrations.append(qa_obj)\n",
    "    if i==20:\n",
    "        break\n",
    "reason_demonstrations[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597539f",
   "metadata": {},
   "source": [
    "**`TODO:`**  \n",
    "\n",
    "- From the **test split** of the dataset, select **100 random samples**.\n",
    "- Preprocess each sample in the same way that you preprocessed the reasoning demonstrations\n",
    "- For each sample, create a new field called `\"Response\"`.  \n",
    "  - `\"Response\"` should be a dictionary with the following keys:  \n",
    "    - `\"Thoughts\"` ‚Üí a list capturing the reasoning steps or intermediate thoughts leading to the solution.  \n",
    "    - `\"Answer\"` ‚Üí the final numeric answer to the problem.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = []\n",
    "for i, row in enumerate(dataset_test.shuffle(seed=42)):\n",
    "    qa_obj = {}\n",
    "    qa_obj['Question'] = row['question']\n",
    "    qa_obj['Response'] = {}\n",
    "    # TODO: Split the answer into thoughts and final answer\n",
    "    qa_obj['Response']['Thoughts'] = row['answer'].split('####')[0].strip().split('\\n')\n",
    "    qa_obj['Response']['Answer'] = row['answer'].split('####')[-1].strip()\n",
    "    test_questions.append(qa_obj)\n",
    "    if i==100:\n",
    "        break\n",
    "test_questions[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d5661",
   "metadata": {},
   "source": [
    "### Generation parameters\n",
    "- Generation parameters control how the model decodes its probability distribution into actual text.\n",
    "- We're gonna look into them extensively during the next lecture and lab session when we break down the text generation task.\n",
    "- For the moment, here's a quick sneak-peak.\n",
    "    - `Max tokens`: Control the maximum length of the model's response.\n",
    "    - `Temperature`: Controls how random or creative the output is (higher = more varied).\n",
    "    - `Top-p`: Limits choices to the most likely words until their probabilities add up to p. Also called Nucleus sampling.\n",
    "\n",
    "In the code below, we define three different configurations of generation parameters. These configurations vary exclusively in temperature, ranging from high (more randomness and creativity) to low (more deterministic and focused).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ecd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"conf1\": {\n",
    "      \"max_tokens\": 512,\n",
    "      \"temperature\": 1,\n",
    "      \"top_p\": 1,\n",
    "    },\n",
    "    \"conf2\": {\n",
    "      \"max_tokens\": 512,\n",
    "      \"temperature\": 0.5,\n",
    "      \"top_p\": 1,\n",
    "    },\n",
    "    \"conf3\": {\n",
    "      \"max_tokens\": 512,\n",
    "      \"temperature\": 0.1,\n",
    "      \"top_p\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf7610e",
   "metadata": {},
   "source": [
    "### üìò GSM8K Math Solver (How It Works)\n",
    "\n",
    "This function builds a structured **prompting pipeline** for solving grade-school math problems (from GSM8K).  \n",
    "Here‚Äôs the idea:\n",
    "\n",
    "1. **System role** ‚Üí Tells the model it is a *mathematical reasoning expert*.  \n",
    "2. **Task instructions** ‚Üí Explain that input comes in JSON (`{\"Question\": \"...\"}`) and output must also be JSON (`{\"Response\": \"...\"}`).  \n",
    "3. **Few-shot examples** ‚Üí Provide sample Q&A pairs so the model learns the pattern.  \n",
    "4. **Actual question** ‚Üí Append the new math problem to solve.  \n",
    "5. **Call the model** ‚Üí Send everything to the AI and get an answer in the required format.  \n",
    "6. **Return results** ‚Üí Output includes the model‚Äôs response (a number in JSON) and how many tokens were used.\n",
    "\n",
    "This design makes the solver‚Äôs answers **consistent, machine-readable, and reliable** for classroom demonstrations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9371f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_question_solver(question, qa_demonstrations, conf='conf1', fewshot=5, n=1):\n",
    "    # Current system prompt\n",
    "    demonstrations = [{\"role\":\"system\", \"content\": \n",
    "                 f'''You are mathemtical reasoning expert whose job is to provide answers to mathematical reasoning questions.'''}]\n",
    "\n",
    "    # Current initial prompt for describing the task\n",
    "    demonstrations.append({\"role\": \"user\", \"content\": '''##Goal \\nProvide an answer to a given mathematical reasoning question.\\n\n",
    "    ##Input \\nYou will be given input in the JSON format as described below\\n\n",
    "    Input format:\n",
    "    {\n",
    "        \"Question\": \"<question>\"\n",
    "    }\n",
    "\n",
    "    ##Output \\nYou should only respond in the JSON format as described below\\n\n",
    "    Output format:\n",
    "    ```json\n",
    "    {\n",
    "        \"Response\": \"<numerical answer>\"\n",
    "    }\n",
    "    ```\n",
    "    Ensure the response can be parsed by Python json.loads\n",
    "    '''})\n",
    "\n",
    "    # Include 5 few-shot demonstrations for ensuring that the model understands the input and output structure\n",
    "    if fewshot:\n",
    "        # always the same demonstrations\n",
    "        for demo in qa_demonstrations[0:fewshot]:\n",
    "            demonstrations.append({\"role\": \"user\", \"content\": f'''{{\"Question\": \"{demo['Question']}\"}}'''})\n",
    "            demonstrations.append({\"role\": \"assistant\", \"content\": f'''{{\"Response\": \"{demo['Response']}\"}}'''})\n",
    "    \n",
    "    messages = []\n",
    "    messages.extend(demonstrations)\n",
    "    messages.append({\"role\": \"user\", \"content\": f'''{{\"Question\": \"{question}\"}}'''})\n",
    "\n",
    "    response = client.chat.completions.create(model= model_name, **payload[conf], messages=messages, n=n)\n",
    "\n",
    "    # # parsing the guesses\n",
    "    response_text = [choice.message.content for choice in response.choices]\n",
    "    return response_text, response.usage.total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b254f6",
   "metadata": {},
   "source": [
    "**`TODO:`** Now that you‚Äôve seen the simple math solver, build a **reasoning solver**.  \n",
    "- First, define the output format you want (just like the math solver did).  \n",
    "- Then, instead of simple Q&A examples, provide **few-shot examples that include reasoning demonstrations** along with the final answer.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50bd88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_question_solver_with_reasoning(question, qa_demonstrations, conf='conf1', fewshot=5, n=1):\n",
    "    # Current system prompt\n",
    "    demonstrations = [{\"role\":\"system\", \"content\": \n",
    "                 f'''You are mathemtical reasoning expert whose job is to provide answers to mathematical reasoning questions.'''}]\n",
    "\n",
    "    # Current initial prompt for describing the task\n",
    "    demonstrations.append({\"role\": \"user\", \"content\": '''##Goal \\nGiven a mathematical reasoning question, you should first think about it primarily by (1) breaking down the problem into steps, (2) reasoning about individual steps, and then (3) combining individual thoughts to come up with a final answer to the question.\\n\n",
    "    ##Input \\nYou will be given input in the JSON format as described below\\n\n",
    "    Input format:\n",
    "    {\n",
    "        \"Question\": \"<question>\"\n",
    "    }\n",
    "\n",
    "    ##Output \\nYou should only respond in the JSON format as described below\\n\n",
    "    Output format:\n",
    "    ```json\n",
    "    {\n",
    "        \"Thoughts\": \"<- short bulleted- list explaining the step-by-step strategy- used to obtain the final answer>\",\n",
    "        \"Answer\": \"<numerical answer>\"\n",
    "    }\n",
    "    ```\n",
    "    Ensure the response can be parsed by Python json.loads\n",
    "    '''})\n",
    "\n",
    "    # TODO: Include few-shot resoning demonstrations for ensuring that the model understands the input and output structure.\n",
    "    if fewshot:\n",
    "        # always the same demonstrations\n",
    "        for demo in qa_demonstrations[0:fewshot]:\n",
    "            demonstrations.append({\"role\": \"user\", \"content\": f'''{{\"Question\": \"{demo['Question']}\"}}'''})\n",
    "            demonstrations.append({\"role\": \"assistant\", \"content\": f'''{{\"Thoughts\": \"{demo['Response']['Thoughts']}\", \"Answer\": \"{demo['Response']['Answer']}\"}}'''})\n",
    "    \n",
    "    # TODO: Create the messages object that includes all demonstrations as well as the current question\n",
    "    messages = []\n",
    "    messages.extend(demonstrations)\n",
    "    messages.append({\"role\": \"user\", \"content\": f'''{{\"Question\": \"{question}\"}}'''})\n",
    "\n",
    "    # TODO: Generate the response from the model\n",
    "    response = client.chat.completions.create(model= model_name, **payload[conf], messages=messages, n=n)\n",
    "\n",
    "    # TODO: Parse the responses to extract the text content\n",
    "    response_text = [choice.message.content for choice in response.choices]\n",
    "    return response_text, response.usage.total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3643ba6",
   "metadata": {},
   "source": [
    "In the code below we provide two helper functions to facilitate your process:\n",
    "\n",
    "- `extract_json_from_response(response)`<br>\n",
    "  This function scans a text response and extracts any valid JSON objects embedded within it.  \n",
    "  It uses a recursive regex pattern to detect balanced curly braces `{...}` and returns a list of JSON strings found.\n",
    "\n",
    "- `is_number(s)`<br>\n",
    "  This function checks if a given string can be interpreted as a number (e.g., integer, float, complex).  \n",
    "  It attempts to safely parse the string using `ast.literal_eval` and returns `True` if successful, otherwise `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_response(response):\n",
    "    '''\n",
    "    test_str = (\"This is a funny text about stuff,\\n\"\n",
    "    \"look at this product {\\\"action\\\":\\\"product\\\",\\\"options\\\":{\\\"action\\\":\\\"product\\\", \\\"action\\\":\\\"product\\\"}}.\\n\"\n",
    "    \"More Text is to come and another JSON string\\n\"\n",
    "    \"{\\\"action\\\":\\\"review\\\",\\\"options\\\":{...}}\")\n",
    "    matches = regex.finditer(pattern, test_str, regex.VERBOSE)\n",
    "    for matchNum, match in enumerate(matches, start=1):\n",
    "\n",
    "    print (\"Match {matchNum} was found at {start}-{end}: {match}\".format(matchNum = matchNum, start = match.start(), end = match.end(), match = match.group()))\n",
    "\n",
    "    for groupNum in range(0, len(match.groups())):\n",
    "        groupNum = groupNum + 1\n",
    "\n",
    "        print (\"Group {groupNum} found at {start}-{end}: {group}\".format(groupNum = groupNum, start = match.start(groupNum), end = match.end(groupNum), group = match.group(groupNum)))\n",
    "    '''\n",
    "    pattern = r\"\"\"\\{(?:[^{}]|(?R))*\\}\"\"\"\n",
    "    matches = regex.finditer(pattern, response, regex.VERBOSE)\n",
    "\n",
    "    groups = [match.group() for matchNum, match in enumerate(matches, start=1)]\n",
    "\n",
    "    return groups\n",
    "\n",
    "# Function to check if a string is a number\n",
    "def is_number(s):\n",
    "    try:\n",
    "        ast.literal_eval(s) # for int, long, float, complex, etc.\n",
    "    except (ValueError, SyntaxError):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdadd6c",
   "metadata": {},
   "source": [
    "### üîÑ The `run_solver` Function\n",
    "\n",
    "This function is a **wrapper** around the math solver that makes it more reliable.  \n",
    "- It retries up to 3 times if something goes wrong.  \n",
    "- It extracts the model‚Äôs JSON output and checks if it contains a valid number.  \n",
    "- If the output is missing or invalid, it returns `\"NA\"`.  \n",
    "- It also tracks the total number of tokens used.  \n",
    "\n",
    "Final return values:  \n",
    "1. **`response`** ‚Üí raw model reply  \n",
    "2. **`answer`** ‚Üí cleaned numeric answer (or `\"NA\"`)  \n",
    "3. **`total_tokens_used`** ‚Üí tokens spent across tries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_solver(question, qa_demonstrations, conf, fewshot=5):\n",
    "    total_tokens_used = 0\n",
    "    num_tries = 0\n",
    "    \n",
    "    while (num_tries < 3):\n",
    "        try:\n",
    "            response, token_used = math_question_solver(question, qa_demonstrations, conf, fewshot)\n",
    "            response_json_obj = extract_json_from_response(response[0])\n",
    "            \n",
    "            total_tokens_used += token_used\n",
    "            num_tries += 1\n",
    "            \n",
    "            if response_json_obj:\n",
    "                response_obj = json.loads(response_json_obj[0])\n",
    "                if \"Response\" in response_obj:\n",
    "                    answer = response_obj[\"Response\"]\n",
    "                    \n",
    "                    if not is_number(answer):\n",
    "                        answer = \"NA\"\n",
    "                    else:\n",
    "                        pass\n",
    "                \n",
    "                else:\n",
    "                    answer = \"NA\"\n",
    "                    print(response_obj)\n",
    "            else:\n",
    "                answer = \"NA\"\n",
    "                print(response_json_obj)\n",
    "            \n",
    "            if answer != \"NA\":\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Encountered an exception {e}, sleeping\")\n",
    "            print(num_tries)\n",
    "            sleep(10)\n",
    "            answer = \"NA\"\n",
    "    \n",
    "    return response, answer, total_tokens_used\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a1c5c",
   "metadata": {},
   "source": [
    "**`TODO:`** Finish the `run_solver_with_reason` function.  \n",
    "- Use the same retry and validation logic as in `run_solver`.  \n",
    "- But this time, extract both `\"Thoughts\"` and `\"Answer\"` from the model‚Äôs JSON output.  \n",
    "- Return the raw response, the thoughts, the final answer, and the total tokens used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bb1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_solver_with_reason(question, qa_demonstrations, conf, fewshot=5):\n",
    "    total_tokens_used =0;\n",
    "    num_tries = 0\n",
    "    \n",
    "    while(num_tries<3):\n",
    "        try:\n",
    "            num_tries += 1\n",
    "            response, tokens_used = math_question_solver_with_reasoning(question, qa_demonstrations, conf, fewshot)\n",
    "            response_json_obj = extract_json_from_response(response[0])\n",
    "            \n",
    "            total_tokens_used += tokens_used\n",
    "             \n",
    "\n",
    "            if response_json_obj:\n",
    "                response_obj = json.loads(response_json_obj[0])\n",
    "                thoughts = response_obj['Thoughts']\n",
    "                answer = response_obj['Answer']\n",
    "                \n",
    "                if not is_number(answer):\n",
    "                    answer='NA'\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                thoughts='NA'; answer='NA'\n",
    "                print(response_json_obj)\n",
    "\n",
    "            if answer!='NA':\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Encountered an exception {e}, sleeping\")\n",
    "            print(num_tries)\n",
    "            sleep(10)\n",
    "            thoughts='NA'; answer='NA'\n",
    "\n",
    "    return response, thoughts, answer, total_tokens_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ce775",
   "metadata": {},
   "source": [
    "### üìä Few-Shot Experiment\n",
    "\n",
    "This loop tests how the solver performs with different numbers of few-shot demonstrations (`0, 1, 5, 10, 20`).  \n",
    "- For each setting, it runs on two test questions.  \n",
    "- It compares the model‚Äôs answer to the human gold-standard answer.  \n",
    "- Results are marked as **Successful** (correct) or **Unsuccessful** (wrong/NA).  \n",
    "- Each run is tested across three configurations (`conf1`, `conf2`, `conf3`).  \n",
    "- All outcomes (question, reasoning, answers, correctness, tokens, etc.) are saved to a CSV file for later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f86cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"generated_answers_without_reasoning_demonstrations\", exist_ok=True)\n",
    "\n",
    "for fewshot in [0, 1, 5, 10, 20]:\n",
    "    print(f\"Few-shot demonstrations: {fewshot}\")\n",
    "    generated_answers = []\n",
    "\n",
    "    for index, row in enumerate(test_questions[:1]):\n",
    "        human_answer = float(row['Response']['Answer'].replace(\",\",\"\"))\n",
    "        \n",
    "        print(f\"Q{index}: {row['Question']}\")\n",
    "        print(f\"Human answer: {human_answer}\")\n",
    "        \n",
    "        for conf in ['conf1', 'conf2', 'conf3']:\n",
    "            response, answer, num_tokens = run_solver(row['Question'], answer_demonstrations, conf, fewshot)\n",
    "\n",
    "            # Answer not found\n",
    "            if answer == \"NA\":\n",
    "                print(answer, human_answer, 'Unsuccessful', num_tokens, payload[conf]['temperature'], fewshot)\n",
    "                generated_answers.append([row['Question'], row['Response']['Thoughts'], human_answer, response, answer, 0, num_tokens, payload[conf]['temperature'], fewshot])\n",
    "\n",
    "            # Correct answer\n",
    "            elif float(answer) == human_answer:\n",
    "                print(float(answer), human_answer, 'Successful', num_tokens, payload[conf]['temperature'], fewshot)\n",
    "                generated_answers.append([row['Question'], row['Response']['Thoughts'], human_answer, response, float(answer), 1, num_tokens, payload[conf]['temperature'], fewshot])\n",
    "\n",
    "            # Incorrect answer\n",
    "            else:\n",
    "                print(float(answer), human_answer, 'Unsuccessful', num_tokens, payload[conf]['temperature'], fewshot)\n",
    "                generated_answers.append([row['Question'], row['Response']['Thoughts'], human_answer, response, float(answer), 0, num_tokens, payload[conf]['temperature'], fewshot])\n",
    "\n",
    "    generated_answers_df = pd.DataFrame(generated_answers, columns=['Question', 'AnswerReasoning_Human', 'Answer_Human', 'Response_LLM', 'Answer_LLM', 'is_correct', 'num_tokens', 'temperature', 'num_demonstrations'])\n",
    "    generated_answers_df.to_csv(f'generated_answers_without_reasoning_demonstrations/{fewshot}.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ad725",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for fewshot in [0, 1, 5, 10, 20]:\n",
    "    df = pd.read_csv(f\"generated_answers_without_reasoning_demonstrations/{fewshot}.csv\")\n",
    "    accuracy = df['is_correct'].mean()\n",
    "    avg_tokens = df['num_tokens'].mean()\n",
    "    results[fewshot] = {\"accuracy\": accuracy, \"avg_tokens\": avg_tokens}\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed781879",
   "metadata": {},
   "source": [
    "**`TODO:`** Write the code to run the same few-shot experiment, but this time using your **reasoning solver** (`run_solver_with_reason`).  \n",
    "- Loop over different numbers of few-shot demonstrations (`0, 1, 5, 10, 20`).  \n",
    "- For each test question and configuration (`conf1`, `conf2`, `conf3`), call the reasoning solver.  \n",
    "- Collect not only the final answer but also the model‚Äôs reasoning steps.  \n",
    "- Compare with the human gold-standard answer and mark results as **Successful** or **Unsuccessful**.  \n",
    "- Save all results (question, human reasoning, human answer, model response, model reasoning, model answer, correctness, tokens, etc.) into a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"generated_answers_with_reasoning_demonstrations\", exist_ok=True)\n",
    "\n",
    "for fewshot in [0, 1, 5, 10, 20]:\n",
    "    print(f'Few-shot demonstrations={fewshot}')\n",
    "    generated_answers = []\n",
    "\n",
    "    for index, row in enumerate(test_questions[:2]):\n",
    "        human_answer = float(row['Response']['Answer'].replace(\",\",\"\"))\n",
    "        \n",
    "        print(f\"Q{index}: {row['Question']}\")\n",
    "        print(f\"Human answer: {human_answer}\")\n",
    "\n",
    "        for conf in ['conf1', 'conf2', 'conf3']:\n",
    "            response, thoughts, answer, num_tokens = run_solver_with_reason(row['Question'], reason_demonstrations, conf, fewshot)\n",
    "\n",
    "            # Answer not found\n",
    "            if answer == \"NA\":\n",
    "                print(answer, human_answer, 'Unsuccessful', num_tokens, payload[conf]['temperature'], fewshot)\n",
    "                generated_answers.append([row['Question'], row['Response']['Thoughts'], human_answer, response, thoughts, answer, 0, num_tokens, payload[conf]['temperature'], fewshot])\n",
    "\n",
    "            # Correct answer\n",
    "            elif float(answer) == human_answer:\n",
    "                print(float(answer), human_answer, 'Successful', num_tokens, payload[conf]['temperature'], fewshot)\n",
    "                generated_answers.append([row['Question'], row['Response']['Thoughts'], human_answer, response, thoughts, float(answer), 1, num_tokens, payload[conf]['temperature'], fewshot])\n",
    "\n",
    "            # Incorrect answer\n",
    "            else:\n",
    "                print(float(answer), human_answer, 'Unsuccessful', num_tokens, payload[conf]['temperature'], fewshot)\n",
    "                generated_answers.append([row['Question'], row['Response']['Thoughts'], human_answer, response, thoughts, float(answer), 0, num_tokens, payload[conf]['temperature'], fewshot])\n",
    "\n",
    "    generated_answers_df = pd.DataFrame(generated_answers, columns=['Question', 'AnswerReasoning_Human', 'Answer_Human', 'Response_LLM', 'AnswerReasoning_LLM', 'Answer_LLM', 'is_correct', 'num_tokens', 'temperature', 'num_demonstrations'])\n",
    "    generated_answers_df.to_csv(f'generated_answers_with_reasoning_demonstrations/{fewshot}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf50f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for fewshot in [0, 1, 5, 10, 20]:\n",
    "    df = pd.read_csv(f\"generated_answers_with_reasoning_demonstrations/{fewshot}.csv\")\n",
    "    accuracy = df['is_correct'].mean()\n",
    "    avg_tokens = df['num_tokens'].mean()\n",
    "    results[fewshot] = {\"accuracy\": accuracy, \"avg_tokens\": avg_tokens}\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad158ecd",
   "metadata": {},
   "source": [
    "**`Practise (Optional):`** Cool, now that you‚Äôve experimented with these solvers using different types of few-shot demonstrations and reasoning strategies, you can take things further on your own!  \n",
    "\n",
    "- **Tweak the prompts** ‚Üí try rephrasing the instructions or adding different examples.  \n",
    "- **Swap models** ‚Üí see how different models handle the same task.  \n",
    "- **Experiment freely** ‚Üí change generation parameters, add constraints, or design creative demonstrations.  \n",
    "\n",
    "The goal is to explore how these choices impact performance and discover what works best for your problem!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
