{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cba48da",
   "metadata": {},
   "source": [
    "\n",
    "## üß© QA (Extractive, Generative) and RAG\n",
    "\n",
    "In this notebook, we‚Äôll explore **Question Answering (QA)** and **Retrieval-Augmented Generation (RAG)** ‚Äî two key paradigms for connecting large language models with factual information.\n",
    "\n",
    "We‚Äôll begin with **Extractive QA**, where a model identifies an answer span directly from a passage using *sequence labeling* with a **BERT-style encoder**.  \n",
    "Then, we‚Äôll move to **Generative QA**, where models like **BART** or **GPT** produce free-form answers in natural language, demonstrating greater flexibility but also higher risk of **hallucination**.\n",
    "\n",
    "Next, we‚Äôll discuss the **limitations** of both approaches ‚Äî extractive QA can be rigid and context-limited, while generative QA may generate fluent but incorrect answers.  \n",
    "To address these issues, we‚Äôll introduce **Retrieval-Augmented Generation (RAG)**, which enriches the model‚Äôs context with relevant external knowledge to improve **factuality** and **reduce hallucinations**.\n",
    "\n",
    "\n",
    "The goal of this notebook is **not just to run QA models**, but to **understand their design trade-offs** and how retrieval-based methods can make generation more trustworthy and grounded in evidence.  \n",
    "\n",
    "By the end of this notebook, you‚Äôll have a clear understanding of:\n",
    "- How extractive and generative QA differ in architecture and behavior,  \n",
    "- Why hallucination occurs in generative systems, and  \n",
    "- How RAG mitigates these issues by integrating retrieval with generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c488ba",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Stanford Question Answering Dataset (**SQuAD**) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
    "\n",
    "SQuAD 1.1 contains 100,000+ question-answer pairs on 500+ articles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e00d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "squad_sample = dataset[\"train\"].select(range(10))\n",
    "\n",
    "#print first 3 examples\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}\")\n",
    "    print(f\"Context: {squad_sample[i]['context'][:100]}...\")\n",
    "    print(f\"Question: {squad_sample[i]['question']}\")\n",
    "    print(f\"Answer: {squad_sample[i]['answers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30a4d65",
   "metadata": {},
   "source": [
    "### Extractive QA Model\n",
    "\n",
    "The class **`AutoModelForQuestionAnswering`** is part of Hugging Face‚Äôs `transformers` library and is specifically designed for **extractive question answering** tasks ‚Äî where the model identifies an answer **span** within a given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad35358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "#you can also explore more samples if you have enough resources\n",
    "#load dataset and pick a case\n",
    "dataset = load_dataset(\"squad\")\n",
    "squad_sample = dataset[\"validation\"].select(range(1000))\n",
    "example = squad_sample[100]\n",
    "context = example[\"context\"]\n",
    "question = example[\"question\"]\n",
    "true_answer = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Ground Truth Answer: {true_answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9385004",
   "metadata": {},
   "source": [
    "`deepset/bert-base-cased-squad2` is a BERT base cased example model trained on SQuAD v2. You can also try different models. The model is based on bert model and designed for Extractive QA\n",
    "\n",
    "For more model details, please refer to this link :https://huggingface.co/deepset/bert-base-cased-squad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d529a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and BERT-style QA model\n",
    "model_name = \"deepset/bert-base-cased-squad2\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6d3fe",
   "metadata": {},
   "source": [
    "----\n",
    "**`TODO:`**\n",
    "\n",
    "1. Feed the inputs into the model to obtain logits that represent how likely each token is the start or end of the answer span.\n",
    "`torch.no_grad()` is used to disable gradient computation during inference.\n",
    "\n",
    "2. Select the tokens with the highest start and end probabilities using `argmax`.\n",
    "3. Extract the predicted answer span from the input IDs and convert it back into natural language using the tokenizer. You can use `tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(...))`\n",
    "4. Display the model‚Äôs predicted answer, the true answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577db8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3bb60bb",
   "metadata": {},
   "source": [
    "----\n",
    "**`TODO:`**\n",
    "\n",
    "Look at the **Example**\n",
    "\n",
    "`Example`:\n",
    "\n",
    "- **Context** Marie Curie was awarded the Nobel Prize in Physics in 1903.\n",
    "\n",
    "- **Question** Did Marie Curie win a Nobel Prize?\n",
    "\n",
    "- **Answer** Yes\n",
    "\n",
    "\n",
    "\n",
    "**Discuss**:  can Extractive QA models still answer correctly? Why or why not?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57696b4",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e4c08b",
   "metadata": {},
   "source": [
    "### Generative QA Model\n",
    "\n",
    "The class **`AutoModelForCausalLM`** (Causal Language Modeling) is part of Hugging Face‚Äôs `transformers` library and is designed for **autoregressive text generation** ‚Äî where each new token is generated **based on all previously generated tokens**.  \n",
    "It is typically used with **decoder-only architectures** such as **GPT-2**, **GPT-Neo**, or **LLaMA**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee9b01",
   "metadata": {},
   "source": [
    "\n",
    "Generative Pre-trained Transformer 2 (GPT-2) is a large language model by OpenAI and the second in their foundational series of GPT models. GPT-2 was pre-trained on a dataset of 8 million web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb3ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "example = squad_sample[100]\n",
    "context = example[\"context\"]\n",
    "question = example[\"question\"]\n",
    "true_answer = example[\"answers\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e7e470",
   "metadata": {},
   "source": [
    "You can make a prompt like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a5619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"Answer the question based only on the given context.\\n\\n\"\n",
    "    f\"Context: {context}\\n\"\n",
    "    f\"Question: {question}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c1199",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "----\n",
    "**`TODO:`**\n",
    "\n",
    "1. **Use the model‚Äôs `generate()` method**  \n",
    "   - Call `model.generate()` to produce new tokens based on your input prompt.  \n",
    "   - Experiment with parameters such as:  \n",
    "     - `max_new_tokens` ‚Üí controls the maximum length of the generated output.  \n",
    "     - `do_sample`, `top_p`, and `temperature` ‚Üí adjust randomness and creativity in generation.  \n",
    "     - `eos_token_id` ‚Üí defines where the model should stop generating (end-of-sequence token).  \n",
    "\n",
    "\n",
    "2. **Convert the model‚Äôs token IDs back into readable text**  \n",
    "   - Use `tokenizer.decode(outputs[0], skip_special_tokens=True)` to transform the generated token IDs into plain text.  \n",
    "   - This step turns the model‚Äôs internal numerical predictions into human-readable language.\n",
    "\n",
    "3. **Extract only the model‚Äôs answer**  \n",
    "   - Remove the original prompt from the decoded text so that you keep just the generated response.  \n",
    "   - This helps you focus on what the model actually ‚Äúanswered,‚Äù rather than the repeated input.\n",
    "\n",
    "4. **Print and compare results**  \n",
    "\n",
    "\n",
    "For more details on how to use `generate()` and `decode()`, refer back to **Exercise 08 ‚Äì Generation**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a34db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4dc9c22",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "**`TODO`**: **Discussion** Run the code multipletimes, does the model‚Äôs generated answer contain any hallucination ‚Äî information that was not stated or implied in the given context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668157f",
   "metadata": {},
   "source": [
    "[Your Answer]\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817207b0",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "RAG improves large language models (LLMs) by incorporating information retrieval before generating responses.\n",
    "\n",
    "RAG helps reduce AI hallucinations by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\n",
    "\n",
    "RAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs.\n",
    "\n",
    "Please refer to Patrick's paper for more details: https://arxiv.org/abs/2005.11401"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b610c",
   "metadata": {},
   "source": [
    "**Retrieval-Augmented Generation (RAG)** combines two key components ‚Äî an **information retriever** and a **text generator** ‚Äî to produce more factual and grounded answers.  \n",
    "\n",
    "Here‚Äôs the standard workflow:\n",
    "\n",
    "1. **Indexing / Document Preparation**  \n",
    "   - Build or load a *knowledge corpus* (e.g., Wikipedia articles, research papers, company documents).  \n",
    "   - Preprocess and store it in a searchable format (using BM25, dense embeddings, or a vector database).\n",
    "\n",
    "2. **Retrieval**  \n",
    "   - When a user asks a question, the retriever finds the top-k most relevant documents from the corpus.  \n",
    "   - Methods can be lexical (**BM25**) or semantic (**embedding-based models** like Sentence-BERT).\n",
    "\n",
    "3. **Context Construction**  \n",
    "   - Combine the retrieved passages into a single context block.  \n",
    "   - Optionally truncate or rank passages based on relevance or confidence scores.\n",
    "\n",
    "4. **Generation**  \n",
    "   - Pass the question + retrieved context to a **generative model**.  \n",
    "   - The model generates an answer *conditioned on both the query and the evidence*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2254d96d",
   "metadata": {},
   "source": [
    "We will continue to use Groq for this task. If you‚Äôre not familiar with it, please refer to **Exercise 07 ‚Äì Post-training**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190673c",
   "metadata": {},
   "source": [
    "Let us try without RAG first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = Groq(\n",
    "    api_key=GROQ_API_KEY,\n",
    ")\n",
    "\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The most recent score between Barcelona and Real Madrid.\",\n",
    "        }\n",
    "    ],\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eafb16",
   "metadata": {},
   "source": [
    "The answer is incorrect because the large language model **does not have access to the most up-to-date knowledge**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd548ec1",
   "metadata": {},
   "source": [
    "### 1. build the index\n",
    "\n",
    "Building an index usually involves preprocessing a corpus (cleaning, tokenizing, or embedding the documents) and then storing them in a searchable structure such as a BM25 index or a vector database for fast retrieval during queries. This is a very small demo dataset ‚Äî the data comes from daily news articles collected from Google.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Real Madrid 2-1 Barcelona (Oct 26, 2025) Game Analysis\",\n",
    "    \"Bill Gates calls for climate fight to shift focus\",\n",
    "    \"Fawlty Towers episode to air on BBC One in tribute to the late Prunella Scales\",\n",
    "    \"Climate Change Falls Over 20% Behind Top Global Concern in 2025\",\n",
    "    \"53.5 of EU services exports by large enterprises\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0b21d",
   "metadata": {},
   "source": [
    "### 2. Retrieval\n",
    "\n",
    "Here, we try to retrieve by `BM25`, BM25 (BM is an abbreviation of best matching) is a ranking function used by search engines to estimate the relevance of documents to a given search query. BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of their proximity within the document. \n",
    "\n",
    "For more information, refer to https://en.wikipedia.org/wiki/Okapi_BM25\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54fe262",
   "metadata": {},
   "source": [
    "You should install BM25 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31423435",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rank_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9db9d7",
   "metadata": {},
   "source": [
    "Here is a Retrieval Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72defc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"[a-zA-Z]+\", text.lower())\n",
    "\n",
    "\n",
    "tokenized_corpus = [tokenize(d) for d in docs]\n",
    "\n",
    "# build the index\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# retrieval function: return top-k documents and their BM25 scores\n",
    "def retrieve_bm25(query, k=1):\n",
    "    q_tokens = tokenize(query)\n",
    "    scores = bm25.get_scores(q_tokens)             \n",
    "    topk_idx = sorted(range(len(scores)), key=lambda i: -scores[i])[:k]\n",
    "    return [(docs[i], float(scores[i])) for i in topk_idx]\n",
    "\n",
    "\n",
    "query = \"What is the score of the latest football match between Real Madrid and Barcelona?\"\n",
    "results = retrieve_bm25(query, k=1)\n",
    "for doc, score in results:\n",
    "    print(f\"Document: {doc}\\nBM25 Score: {score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30274e",
   "metadata": {},
   "source": [
    "### 3. Context Construction\n",
    "\n",
    "----\n",
    "\n",
    "**`TODO:`**: \n",
    "\n",
    "You should write a prompt that includes not only the question but also the context information. \n",
    "\n",
    "This helps the model ground its answer in the provided evidence instead of relying on memorized knowledge.\n",
    "\n",
    "A well-structured prompt makes it clear what information the model should use and what task it should perform.\n",
    "\n",
    "In RAG, including both the context and the question ensures the model produces factually accurate, context-aware answers.\n",
    "\n",
    "If you‚Äôre not familiar with writing prompts, please refer to **Exercise 07 ‚Äì Post-training**.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64982585",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 4. **Generation**  \n",
    "   \n",
    "Pass the question + retrieved context to a **generative model**.  \n",
    "\n",
    "----\n",
    "**`ToDo:`**: Construct a new request using our custom prompt\n",
    "\n",
    "**Goal**: Combine the retrieved context from BM25 and the query into a single prompt,\n",
    "then send it to the model for generation.\n",
    "\n",
    "You can follow the code at the begining of RAG section: ` client.chat.completions.create(...)`\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb83e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp25-exam",
   "language": "python",
   "name": "exam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
