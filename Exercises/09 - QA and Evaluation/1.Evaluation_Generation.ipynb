{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "456dfd6a",
   "metadata": {},
   "source": [
    "## üß© Evaluation of Text Generation\n",
    "\n",
    "In this notebook, we'll continue **look at text generation**. And this time, we will focus on **Evaluation**.\n",
    "\n",
    "Evaluation of Text Generation is a central challenge in natural language generation research. How we evaluate systems shapes not only model comparison but also our very definition of what counts as a ‚Äúgood‚Äù output. In this session, you will explore different types of evaluation metrics to uncover their strengths, limitations, and inherent biases.\n",
    "\n",
    "The goal of this notebook is **not to chase perfect evaluation scores**, but to **experiment** and **build intuition** about how evaluation of text generation actually works.  \n",
    "\n",
    "You‚Äôre encouraged to:  \n",
    "- Try out different evaluation metrics from various classes,  \n",
    "- Compare how they rate the same model outputs, and  \n",
    "- Reflect on when and why these metrics agree‚Äîor fail to.  \n",
    "\n",
    "In this notebook, we‚Äôll focus on two main types of evaluation metrics:  \n",
    "(a) **Content-overlap metrics**   \n",
    "(b) **Model-based metrics**.  \n",
    "\n",
    "Beyond these automatic methods, you‚Äôre also encouraged to **manually evaluate** some generated outputs yourself ‚Äî observe which metrics best align with your own intuition about quality.\n",
    "\n",
    "By the end of this notebook, you‚Äôll have a practical understanding of how to **evaluate text generation models** and a clearer sense of **what good evaluation really means**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb0d97",
   "metadata": {},
   "source": [
    "### üßÆ The `evaluate` Library\n",
    "\n",
    "[`evaluate`](https://huggingface.co/docs/evaluate) is a lightweight library from Hugging Face that provides a unified interface for computing a wide range of NLP evaluation metrics ‚Äî from classic ones like **BLEU**, and **Perplexity**, to modern model-based metrics such as **BERTScore** and **COMET**. \n",
    "\n",
    " Evaluate provides access to a wide range of evaluation tools. It covers a range of modalities such as text, computer vision, audio, etc. as well as tools to evaluate models or datasets. \n",
    "\n",
    "You can check more Metrics here: https://huggingface.co/evaluate-metric/spaces\n",
    "\n",
    " Each metric is a separate Python module, but for using any of them, there is a single entry point: `evaluate.load()`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3e8db",
   "metadata": {},
   "source": [
    "### üìè Content-overlap Metrics\n",
    "\n",
    "**Content-overlap metrics** evaluate how closely a generated text matches one or more reference texts by comparing their surface forms ‚Äî typically through word or n-gram overlap.  \n",
    "\n",
    "These metrics are simple, interpretable, and fast to compute, but they often fail to capture deeper semantic meaning or paraphrasing.\n",
    "\n",
    "In this notebook, we‚Äôll focus on two of the most widely used metrics in this category:  \n",
    "\n",
    "- **BLEU** ‚Äî computes the overlap of n-grams between generated and reference texts, and is widely used in **machine translation** and **summarization**.  \n",
    "  \n",
    "- **ROUGE** ‚Äî measures the overlap of n-grams, words, or word sequences, but is especially designed for **summarization** tasks, focusing on recall rather than precision.n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a162333",
   "metadata": {},
   "source": [
    "### BLEU\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine‚Äôs output and that of a human: ‚Äúthe closer a machine translation is to a professional human translation, the better it is‚Äù ‚Äì this is the central idea behind BLEU. \n",
    "\n",
    "BLEU and BLEU-derived metrics are most often used for machine translation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b7b31",
   "metadata": {},
   "source": [
    "You should first run `pip install evaluate` to install it. Then, You Can load evaluate by this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e510349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde4ed7",
   "metadata": {},
   "source": [
    "Here is an example texts:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11583e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\n",
    "    \"The cat is on the mat.\",\n",
    "    \"There is a cat sitting on the carpet.\"\n",
    "]\n",
    "references = [\n",
    "    [\"The cat sits on the mat.\"],\n",
    "    [\"A cat is on the carpet.\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64191865",
   "metadata": {},
   "source": [
    "------------\n",
    "**`TODO:`** Compute the BLEU score for the `predictions` list against the `references` list using the `bleu.compute()` function.  \n",
    "- Use the `predictions` variable as the input for the `predictions` parameter.  \n",
    "- Use the `references` variable as the input for the `references` parameter.  \n",
    "- Store the result in a variable named `results`.  \n",
    "- Print the BLEU score from the `results` dictionary using the key `'bleu'`.  \n",
    "This will help you understand how BLEU evaluates the overlap between the generated and reference texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93859d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9cbf80e",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e2f53f",
   "metadata": {},
   "source": [
    "BLEU also has several limitations, which we‚Äôll illustrate through examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20aeb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [[\"The cat is on the mat.\"]]\n",
    "predictions_good = [\"A cat sits on the rug.\"]    \n",
    "predictions_bad = [\"The cat is not on the mat.\"]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb56515",
   "metadata": {},
   "source": [
    "----\n",
    "**`TODOÔºö`**: Compute the scores of `predictions_good` and `predictions_bad` against the `references`, and **discuss** why this happens ‚Äî what limitation of BLEU does it reveal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ff377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f29b43f8",
   "metadata": {},
   "source": [
    "[Your Answer]\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981aa1e",
   "metadata": {},
   "source": [
    "### ROUGE\n",
    "\n",
    "ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation. ROUGE metrics range between 0 and 1, with higher scores indicating higher similarity between the automatically produced summary and the reference.\n",
    "\n",
    "\n",
    "\n",
    "This metrics is a wrapper around Google Research reimplementation of ROUGE: https://github.com/google-research/google-research/tree/master/rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de364f",
   "metadata": {},
   "source": [
    "Unlike BLEU, which focuses on precision, ROUGE emphasizes **recall** ‚Äî how much of the reference text‚Äôs content is captured in the generated text.\n",
    "\n",
    "Here are the main variants you‚Äôll encounter:\n",
    "\n",
    "- **ROUGE-1** ‚Äî Measures the overlap of individual words (unigrams) between the prediction and reference.  \n",
    "  ‚Üí Captures basic lexical similarity.\n",
    "\n",
    "- **ROUGE-2** ‚Äî Measures the overlap of 2-word sequences (bigrams).  \n",
    "  ‚Üí Reflects fluency and short-phrase consistency.\n",
    "\n",
    "- **ROUGE-L** ‚Äî Based on the *Longest Common Subsequence (LCS)* between prediction and reference.  \n",
    "  ‚Üí Captures sentence-level structure and word order similarity.\n",
    "\n",
    "- **ROUGE-Lsum** ‚Äî A summary-level variant of ROUGE-L that averages the LCS-based recall across multiple sentences in the generated summary.  \n",
    "  ‚Üí More suitable for multi-sentence summarization tasks.\n",
    "\n",
    "üí° **Interpretation:**  \n",
    "Higher ROUGE scores generally indicate better content overlap with the reference, but like BLEU, ROUGE is still surface-based and does not measure semantic correctness or factual accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471262e8",
   "metadata": {},
   "source": [
    "Here is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a37dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"The cat is on the mat.\"\n",
    "\n",
    "prediction_normal = \"The cat is on the mat.\"\n",
    "prediction_paraphrase = \"A cat sits on the rug.\"\n",
    "prediction_negated = \"The cat is not on the mat.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce93f1",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**`TODO:`** Use the example above to compute **ROUGE** scores for your generated outputs.\n",
    "\n",
    "\n",
    "\n",
    "1. **Load the ROUGE metric** using `evaluate.load(\"rouge\")`.\n",
    "\n",
    "2. **Compute ROUGE** for your different predictions (e.g., `prediction_normal`, `prediction_paraphrase`, and `prediction_negated`) against the same reference.\n",
    "\n",
    "3. **Access specific ROUGE variants** such as ROUGE-1, ROUGE-2, ROUGE-L, or ROUGE-Lsum by indexing the result dictionary (e.g., `rouge.compute(...)[‚Äòrouge1‚Äô]`).\n",
    "\n",
    "4. **Compare the results** across the three predictions:  \n",
    "   - How do the scores differ between exact matches, paraphrases, and negated sentences?  \n",
    "   - Do higher ROUGE scores always correspond to better or more semantically accurate outputs?\n",
    "\n",
    "5. **Discuss your findings:**  \n",
    "   Consider where ROUGE may fail to capture semantic equivalence or meaning preservation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b64c173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15df21f7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a47a92c",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6fc57",
   "metadata": {},
   "source": [
    "### Model-based metrics: bert_score\n",
    "\n",
    "**BERTScore** is a *model-based evaluation metric* that measures the similarity between generated and reference texts using contextual embeddings from pretrained language models such as **BERT** or **RoBERTa**.  \n",
    "\n",
    "Instead of comparing surface-level n-gram overlap (like BLEU or ROUGE), BERTScore computes **semantic similarity** between words by aligning their embeddings in a high-dimensional space.  It captures meaning even when different words or phrases are used.\n",
    "\n",
    "\n",
    "üí° **Note:**  \n",
    "BERTScore relies on a large pretrained model, so it is computationally heavier than BLEU or ROUGE, but it provides a more meaning-aware evaluation of generated text.\n",
    "\n",
    "You can refer to Tianyi‚Äôs paper for more details (but unfortunately, it‚Äôs a different Tianyi ‚Äî not me ü§°): https://arxiv.org/pdf/1904.09675\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e9ffb8",
   "metadata": {},
   "source": [
    "We still use the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0634e38f",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "reference = \"The cat is on the mat.\"\n",
    "\n",
    "prediction_paraphrase = \"A cat sits on the rug.\"\n",
    "prediction_negated = \"The cat is not on the mat.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a30a9f",
   "metadata": {},
   "source": [
    "BERTScore uses contextual embeddings from a pretrained model (like BERT) to measure **semantic similarity** between tokens in the prediction and reference sentences.  \n",
    "Here‚Äôs how each component is computed:\n",
    "\n",
    "1. **Token-level similarity:**  \n",
    "   Each token is represented as a vector embedding.  \n",
    "   The similarity between two tokens is measured using **cosine similarity**.\n",
    "\n",
    "2. **Precision (P):**  \n",
    "   For each token in the *prediction*, find the **most similar** token in the *reference*,  \n",
    "   then take the **average** of these maximum similarities.  \n",
    "\n",
    "\n",
    "3. **Recall (R):**  \n",
    "   For each token in the *reference*, find the **most similar** token in the *prediction*,  \n",
    "   then take the average of these maximum similarities.  \n",
    "\n",
    "4. **F1 Score:**  \n",
    "   The harmonic mean of Precision and Recall, capturing overall alignment between prediction and reference\n",
    "\n",
    "\n",
    "üí° **Intuition:**  \n",
    "- Precision measures how *relevant* the generated tokens are to the reference.  \n",
    "- Recall measures how much of the reference meaning is *covered* by the generation.  \n",
    "- F1 balances both ‚Äî higher F1 indicates stronger semantic similarity overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cdb5a3",
   "metadata": {},
   "source": [
    "----\n",
    "**`TODO:`** Use the example above to compute **BERTScore** for your generated outputs.\n",
    " You can access specific values via keys like `res['precision']`, `res['recall']`, and `res['f1']` (e.g., `res['f1'][0]`). you can use any model_type settings (e.g., `model_type=\"bert-base-uncased\"`) and compare scores with BLEU and ROUGE, which aligns with your intuition?\n",
    "\n",
    " ‚úÖ Steps to follow:\n",
    "\n",
    "1. **Load the BERTScore metric** using `evaluate.load(\"bertscore\")`.\n",
    "\n",
    "2. **Compute BERTScore** for your predictions (e.g., `prediction_normal`, `prediction_paraphrase`, and `prediction_negated`) against the same reference.  \n",
    "   You can experiment with different model backbones, such as `model_type=\"bert-base-uncased\"` or `model_type=\"roberta-large\"`.\n",
    "\n",
    "3. **Access individual score components** using keys like  \n",
    "   `res['precision']`, `res['recall']`, and `res['f1']`  \n",
    "   (for example, `res['f1'][0]` to view the F1 score of a single instance).\n",
    "\n",
    "4. **Compare the results** across the three predictions and reflect on:  \n",
    "   - Which metric‚Äî**BLEU**, **ROUGE**, or **BERTScore**‚Äîbest matches your own intuition about semantic similarity?  \n",
    "   - Are there cases where BERTScore provides a more meaningful evaluation than surface-level metrics?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a954fb",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69b299d1",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c132a9",
   "metadata": {},
   "source": [
    "-----\n",
    "**`TODO:`** : Discussion: Why does `prediction_negated` still get a high score? How to imporve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc85c9e",
   "metadata": {},
   "source": [
    "[Your Answer]\n",
    "\n",
    "\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp25-exam",
   "language": "python",
   "name": "exam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
