{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8011eb1c",
   "metadata": {},
   "source": [
    "# üß† Text Generation with Hugging Face Transformers\n",
    "\n",
    "In this notebook, we'll take a **first look at text generation** using Hugging Face's ü§ó `transformers` library.\n",
    "\n",
    "Modern language models, such as GPT-style models, can **generate coherent text continuations** given a prompt.  \n",
    "However, the way they generate text can vary dramatically depending on the *generation strategy* used, such as:\n",
    "\n",
    "- **Greedy decoding** ‚Äì always picks the most likely next token.  \n",
    "- **Sampling** ‚Äì randomly samples from possible next tokens.  \n",
    "- **Top-k / Top-p sampling** ‚Äì limits the candidate pool for more diverse or focused generations.\n",
    "\n",
    "We‚Äôll use the [`transformers.GenerationConfig`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig) class to easily control and compare these settings.\n",
    "\n",
    "The goal of this notebook is **not to complete exercises**, but to **experiment** and **develop intuition** about how generation works.  \n",
    "You‚Äôre encouraged to:\n",
    "- Play around with different configuration parameters,  \n",
    "- Observe how they affect the model‚Äôs output, and  \n",
    "- Examine the output of the model at different stages.\n",
    "\n",
    "In the **next notebook**, we‚Äôll take a **deeper dive** into the different generation strategies themselves, exploring how methods like greedy search, beam search, and sampling work under the hood and how they impact the final output.\n",
    "\n",
    "By the end of this notebook, you‚Äôll have a solid foundation for experimenting with text generation and preparing to analyze decoding strategies in more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0507973",
   "metadata": {},
   "source": [
    "## üß© Loading a Pretrained Model\n",
    "\n",
    "We‚Äôll first load a pretrained model and tokenizer that can perform text generation.\n",
    "\n",
    "We'll use **DistilGPT-2**, a lightweight version of GPT-2, which runs easily on most laptops and Colab environments.\n",
    "\n",
    "We'll use `AutoTokenizer` and `AutoModelForCausalLM`. These automatically select the right tokenizer/model classes for causal (left-to-right) language modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425fdea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6795f",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Basic Text Generation\n",
    "\n",
    "Let‚Äôs start by generating text *without* changing any generation settings.\n",
    "\n",
    "We'll use `model.generate()` with default parameters to see how the model continues a prompt like:\n",
    "\n",
    "> \"Once upon a time\"\n",
    "\n",
    "The default configuration typically uses *greedy decoding*, which means the model always chooses the most probable next token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e59ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time of war, the United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Once upon a time\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe51a5e",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è GenerationConfig\n",
    "- `GenerationConfig` is a Hugging Face class that defines **how a model generates text**. It acts as a central configuration for text generation parameters.  \n",
    "- It controls aspects of decoding such as **maximum length**, **sampling strategy**, **temperature**, **top-k**, and **top-p** values.  \n",
    "- Instead of passing these parameters directly to `model.generate()`, you can store them neatly inside a `GenerationConfig` object for cleaner, reusable code.\n",
    "\n",
    "For further documentation regarding HF dataset, you are encouraged to explore the following [documentation](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig).\n",
    "\n",
    "üìò **Tip:** You can inspect a model‚Äôs default generation configuration at any time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81bc8e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c37391",
   "metadata": {},
   "source": [
    "### üïí Length & Stopping Criteria\n",
    "\n",
    "These arguments control **how long the model continues generating text** and **what conditions cause it to stop**.  \n",
    "They help ensure the output is the right length and prevent the model from generating endlessly.\n",
    "\n",
    "- **`max_new_tokens`** ‚Äî Sets the *maximum number of new tokens* the model is allowed to generate after the input prompt.  \n",
    "  This defines the upper bound of the output length.\n",
    "\n",
    "- **`min_new_tokens`** ‚Äî Specifies the *minimum number of new tokens* the model must produce before it can stop.  \n",
    "  This prevents the generation from ending too early.\n",
    "\n",
    "- **`max_time`** ‚Äî Limits the total generation time (in seconds).  \n",
    "  Useful for ensuring that generation doesn‚Äôt run indefinitely for complex prompts or large models.\n",
    "\n",
    "- **`stop_strings`** ‚Äî Defines one or more text sequences that immediately stop generation when produced.  \n",
    "  Commonly used to signal the end of structured outputs (e.g., ‚ÄúEND‚Äù, ‚ÄúUser:‚Äù, or special markers in dialogues).\n",
    "\n",
    "Together, these parameters give you fine-grained control over the **duration and stopping behavior** of text generation, allowing you to tailor output length and prevent runaway completions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac24a7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time of war, the United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the\n"
     ]
    }
   ],
   "source": [
    "# Control how long the model generates text and when it stops\n",
    "length_config = GenerationConfig(\n",
    "    max_new_tokens=40,          # limit new tokens generated\n",
    "    min_new_tokens=10,          # ensure at least 10 new tokens\n",
    "    max_time=5.0,               # stop after 5 seconds if still running\n",
    "    stop_strings=[\"THE END\"]    # stop when model outputs this string\n",
    ")\n",
    "\n",
    "output = model.generate(**inputs, generation_config=length_config, tokenizer=tokenizer)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca7f1ca",
   "metadata": {},
   "source": [
    "### üé≤ Sampling & Logits Control\n",
    "\n",
    "These parameters control **how the model handles repetition, forbidden words, and special tokens** during generation.  \n",
    "They allow you to guide the model toward more natural, diverse, and constrained outputs.\n",
    "\n",
    "- **`repetition_penalty`** ‚Äî Penalizes the probability of tokens that have already appeared in the generated text.  \n",
    "  Increasing this value discourages the model from repeating words or phrases too often, leading to more varied sentences.\n",
    "\n",
    "- **`no_repeat_ngram_size`** ‚Äî Prevents the model from repeating any *n-gram* (sequence of *n* words) that has already been generated.  \n",
    "  For example, setting this to 2 ensures that no two-word sequence appears twice, reducing redundancy.\n",
    "\n",
    "- **`bad_words_ids`** ‚Äî Defines specific tokens or sequences that the model is not allowed to generate.  \n",
    "  This is useful for filtering out unwanted terms, sensitive content, or domain-irrelevant vocabulary.\n",
    "\n",
    "- **`forced_bos_token_id`** ‚Äî Forces the model to begin generation with a specific *beginning-of-sequence (BOS)* token.  \n",
    "  Ensures a consistent starting point for all outputs.\n",
    "\n",
    "- **`forced_eos_token_id`** ‚Äî Forces the model to include a specific *end-of-sequence (EOS)* token when finishing.  \n",
    "  Guarantees that all generations end cleanly and can be properly decoded.\n",
    "\n",
    "Together, these settings provide fine control over **output quality and structure**, helping to reduce repetition, enforce constraints, and maintain coherent beginnings and endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0fb634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time of war, the United States was in danger. The Soviet Union had been on its way to\n"
     ]
    }
   ],
   "source": [
    "sampling_config = GenerationConfig(\n",
    "    repetition_penalty=1.2,      # discourage repeating the same words\n",
    "    no_repeat_ngram_size=2,      # prevent repeating any 2-word sequences\n",
    "    bad_words_ids=[[tokenizer.encode(\"boring\")[0]]],  # forbid the word \"boring\"\n",
    "    forced_bos_token_id=tokenizer.bos_token_id,       # ensure BOS token is added\n",
    "    forced_eos_token_id=tokenizer.eos_token_id        # ensure EOS token is added\n",
    ")\n",
    "\n",
    "output = model.generate(**inputs, generation_config=sampling_config)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3dfb0",
   "metadata": {},
   "source": [
    "### üì§ Output & Return Options\n",
    "\n",
    "These arguments control **what information is returned** from the generation process and **how it is formatted**.  \n",
    "They are useful for analyzing the model‚Äôs internal behavior or generating multiple variations of a prompt.\n",
    "\n",
    "- **`num_return_sequences`** ‚Äî Specifies how many independent generations the model should produce for the same input.  \n",
    "  This allows you to compare multiple possible completions and choose the best one.\n",
    "\n",
    "- **`do_sample`** ‚Äî Enables random sampling rather than deterministic decoding.  \n",
    "  When active, the model can generate more diverse outputs by introducing controlled randomness.\n",
    "\n",
    "- **`output_scores`** ‚Äî Returns the token-level probability scores assigned during generation.  \n",
    "  These scores can be used to analyze how confident the model was about each generated token.\n",
    "\n",
    "- **`output_attentions`** ‚Äî Returns the attention weights from the model‚Äôs layers.  \n",
    "  Useful for understanding which parts of the input the model focused on while generating each token.\n",
    "\n",
    "- **`output_hidden_states`** ‚Äî Returns the hidden state representations for each layer.  \n",
    "  This provides insight into how the model‚Äôs internal representations evolve over time.\n",
    "\n",
    "- **`output_logits`** ‚Äî Returns the raw, unnormalized prediction values (*logits*) for each token before the softmax step.  \n",
    "  Helpful for debugging or inspecting the model‚Äôs raw output distribution.\n",
    "\n",
    "- **`return_dict_in_generate`** ‚Äî When set to `True`, returns a dictionary-like object containing all outputs  \n",
    "  (generated sequences, scores, attentions, hidden states, logits, etc.) instead of just the token IDs.  \n",
    "  This makes it easier to access and analyze different components of the generation process.\n",
    "\n",
    "Together, these options make it possible to not only **generate text**, but also to **inspect, compare, and interpret** how the model arrived at its output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1292ab75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Output 1 ---\n",
      "Once upon a time of darkness a powerful and powerful force would destroy the planet and cause a tidal wave that would obliterate\n",
      "\n",
      "--- Output 2 ---\n",
      "Once upon a time of war, the world became divided into two parts, which were divided into the nations of the seven\n",
      "\n",
      "--- Output 3 ---\n",
      "Once upon a time of war the United States would cease its military activities in Iraq, and would maintain its occupation until the\n",
      "\n",
      "Available output keys: odict_keys(['sequences', 'scores', 'logits', 'attentions', 'hidden_states', 'past_key_values'])\n"
     ]
    }
   ],
   "source": [
    "# Define a generation configuration\n",
    "output_config = GenerationConfig(\n",
    "    num_return_sequences=3,         # generate 3 independent completions\n",
    "    do_sample=True,                 # Activating random sampling (Will visit this in the next notebook)\n",
    "    output_scores=True,             # return token-level scores\n",
    "    output_attentions=True,         # (optional) set True to inspect attention\n",
    "    output_hidden_states=True,      # (optional) set True to inspect hidden states\n",
    "    output_logits=True,             # return raw logits for analysis\n",
    "    return_dict_in_generate=True    # return a dictionary instead of just IDs\n",
    ")\n",
    "\n",
    "# Generate text using the custom configuration\n",
    "generation = model.generate(**inputs, generation_config=output_config)\n",
    "\n",
    "for i, seq in enumerate(generation.sequences):\n",
    "    print(f\"--- Output {i+1} ---\")\n",
    "    print(tokenizer.decode(seq, skip_special_tokens=True))\n",
    "    print()\n",
    "\n",
    "# Access additional returned information\n",
    "print(\"Available output keys:\", generation.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd9df77",
   "metadata": {},
   "source": [
    "`**TODO:**` Examine all output keys other than `sequences`. These correspond to concepts we've already covered in lectures and exercises. This is your chance to explore them in a practical way: try printing their values, types, and shapes (or anything else) to build a deeper understanding and intuition.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
