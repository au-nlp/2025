{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c439fcf",
   "metadata": {},
   "source": [
    "# ‚ú® Basic decoding methods\n",
    "\n",
    "In text generation, decoding strategies determine how a model chooses the next token in a sequence. Different strategies can lead to outputs that vary in coherence, creativity, and diversity. Understanding these methods helps you control how models generate text and tailor their behavior to different tasks.  \n",
    "\n",
    "In this notebook, you‚Äôll explore and compare several generation strategies to see how these choices shape the final output. üöÄ  \n",
    "\n",
    "**üôè Acknowledgment:** This notebook draws inspiration, explanations, and graphics from the excellent Hugging Face blog post [*‚ÄúHow to generate text‚Äù*](https://huggingface.co/blog/how-to-generate) by [Patrick von Platen](https://huggingface.co/patrickvonplaten). Huge thanks to the author and the Hugging Face team for their amazing open resources üíôü§ó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05475a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "import torch\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(torch_device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3d320",
   "metadata": {},
   "source": [
    "## Greedy search\n",
    "\n",
    "Greedy search is a decoding strategy used in autoregressive text generation.  \n",
    "At each time step, the model selects the token with the **highest conditional probability** given the tokens generated so far.  \n",
    "\n",
    "In formula form:\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "w_t = \\arg\\max_{w} P(w \\mid w_{1:t-1})\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "where \\( w_{1:t-1} \\) are the previously generated tokens. \n",
    "\n",
    "\n",
    "### Example\n",
    "<center><img src=\"./assets/greedy_search.png\"/></center>\n",
    "\n",
    "Starting from the initial word **\"The\"**,  \n",
    "the algorithm greedily chooses the next word with the highest probability at each step:\n",
    "\n",
    "| Step | Current sequence | Next word (highest probability) | Resulting sequence |\n",
    "|------|------------------|----------------------------------|--------------------|\n",
    "| 1 | \"The\" | \"nice\" | \"The nice\" |\n",
    "| 2 | \"The nice\" | \"woman\" | \"The nice woman\" |\n",
    "\n",
    "So the final generated sequence is:\n",
    "\n",
    "`(\"The\", \"nice\", \"woman\")`\n",
    "\n",
    "If the probabilities were:\n",
    "\n",
    "- P(\"nice\" | \"The\") = 0.5  \n",
    "- P(\"woman\" | \"The nice\") = 0.4  \n",
    "\n",
    "then the overall sequence probability is:\n",
    "\n",
    "`0.5 √ó 0.4 = 0.2`\n",
    "\n",
    "This example shows how greedy search **selects the most likely token at each step**.\n",
    "\n",
    "`**TODO (Discussion):**` What are your thoughts on Greedy Search? What limitations might it have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957a1d6",
   "metadata": {},
   "source": [
    "### General comments about Greedy Search\n",
    "- **Simple and fast** ‚Äî requires minimal computation.\n",
    "- Works well for tasks needing consistent, reliable completions.\n",
    "\n",
    "### Limitations\n",
    "- **Shortsightedness:** Only optimizes locally, not globally.  \n",
    "- **Repetition or collapse:** Can generate repetitive or looping text.  \n",
    "- **Not globally optimal:** The final sequence might not have the highest overall probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9cecbc",
   "metadata": {},
   "source": [
    "### Code\n",
    "Let's have a look at how greedy search can be used in `transformers`. All generation parameters are set to their default values, as greedy search is the default search method already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "045e3e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(torch_device)\n",
    "\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57430345",
   "metadata": {},
   "source": [
    "`**TODO (Discussion)**:` Do you see something wrong with the generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76454034",
   "metadata": {},
   "source": [
    "The model keeps generating the same sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f6f05",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "Beam search is a decoding strategy that keeps track of several possible sequences (called **beams**) at each generation step.  \n",
    "Instead of picking only the single most likely next word (as in greedy search), beam search explores multiple alternatives in parallel and selects the **overall most probable sequence** at the end.\n",
    "\n",
    "### How It Works\n",
    "1. Start with the initial token(s) ‚Äî for example, `\"The\"`.\n",
    "2. Generate a list of the top `k` most likely next words (where `k` is the **beam width**).\n",
    "3. For each candidate, keep expanding one token at a time.\n",
    "4. At each step, keep only the `k` sequences with the highest overall probability.\n",
    "5. When generation stops (e.g., after an end token or max length), the sequence with the **highest total probability** is selected.\n",
    "\n",
    "\n",
    "### Example\n",
    "<center><img src=\"./assets/beam_search.png\"/></center>\n",
    "\n",
    "Starting from the initial word **\"The\"**,  \n",
    "the model predicts the next tokens and expands possible continuations.  \n",
    "We‚Äôll use a **beam width of 2**.\n",
    "\n",
    "| Step | Candidate sequences | Probabilities | Top beams kept |\n",
    "|------|---------------------|----------------|----------------|\n",
    "| 1 | \"The nice\" (0.5), \"The dog\" (0.4), \"The car\" (0.1) | Keep 2 beams ‚Üí \"The nice\", \"The dog\" | \"The nice\" (0.5), \"The dog\" (0.4) |\n",
    "| 2 | From \"The nice\":<br>‚Ä¢ \"The nice woman\" (0.5 √ó 0.4 = 0.20)<br>‚Ä¢ \"The nice house\" (0.5 √ó 0.3 = 0.15)<br>‚Ä¢ \"The nice guy\" (0.5 √ó 0.3 = 0.15)<br>From \"The dog\":<br>‚Ä¢ \"The dog has\" (0.4 √ó 0.9 = 0.36)<br>‚Ä¢ \"The dog runs\" (0.4 √ó 0.05 = 0.02)<br>‚Ä¢ \"The dog and\" (0.4 √ó 0.05 = 0.02) | Keep 2 best beams ‚Üí \"The dog has\" (0.36), \"The nice woman\" (0.20) | \"The dog has\" (0.36), \"The nice woman\" (0.20) |\n",
    "\n",
    "The final output is:\n",
    "\n",
    "`(\"The dog has\")`  \n",
    "with an overall probability of `0.36`.\n",
    "\n",
    "This example shows how beam search keeps **multiple candidates** at each step and chooses the sequence with the **highest overall probability**, instead of the locally best option at each token.\n",
    "\n",
    "### Code\n",
    "Let's see how we implement this in `transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f653fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I'm not sure\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,            # Activate beam search with 5 beams\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68fbd41",
   "metadata": {},
   "source": [
    "`**TODO (Discussion):**` What are your thoughts on Beam Search? Does it overcome the limitations of Greedy Search and if so does it have any limitations of its own? If you're not sure yet, have a look at the next code block which might be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7cf29e",
   "metadata": {},
   "source": [
    "### Why Use It?\n",
    "- Balances **exploration** and **efficiency**: it‚Äôs less myopic than greedy search but still tractable.  \n",
    "- Produces more **globally optimal** sequences.  \n",
    "- Works well for structured tasks like translation, summarization, and captioning.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "- **Computationally heavier** than greedy search (more sequences tracked).  \n",
    "- May still favor high-probability but **less diverse** outputs.  \n",
    "- The result can depend strongly on the **beam width** ‚Äî too small ‚Üí misses good sequences; too large ‚Üí slows down or overfits to longer outputs.\n",
    "- The repetition problem still persists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4913d540",
   "metadata": {},
   "source": [
    "## Avoiding Repetition with N-gram Penalties\n",
    "\n",
    "While beam search often produces more fluent and coherent results than greedy search, it can still suffer from **repetitions** ‚Äî where the same phrases or word sequences are generated multiple times.  \n",
    "This happens because beam search tends to favor high-probability continuations, which can lead the model to repeat familiar patterns rather than explore new ones.\n",
    "\n",
    "A common remedy for this issue is to apply **n-gram penalties**.\n",
    "An **n-gram** is simply a sequence of *n* words (for example, a 2-gram or ‚Äúbigram‚Äù could be `\"the cat\"`, `\"in the\"`, etc.).  \n",
    "The **n-gram penalty** ensures that no identical n-gram appears more than once in the generated text.\n",
    "\n",
    "In practice, this is done by **setting the probability of any next word that would recreate an existing n-gram to zero**.  \n",
    "This discourages the model from repeating the same word sequences, leading to more diverse and natural outputs.\n",
    "\n",
    "`**TODO:**` Let‚Äôs try this out! Create a generation in the same way as before (can be greedy or beam search), but update it by using the setting: `no_repeat_ngram_size = 2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e48bcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n"
     ]
    }
   ],
   "source": [
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2, # Activate n-gram penalization with n=2\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b707cad5",
   "metadata": {},
   "source": [
    "`**TODO (Discussion):**` Can you think of a case or application where this penalization approach could be proven problematic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ce2f6",
   "metadata": {},
   "source": [
    "N-gram penalties are effective in reducing repetition, but they need to be used carefully.  \n",
    "By blocking the model from repeating any n-gram, we may **accidentally remove legitimate repetitions** that are important for meaning or fluency.\n",
    "\n",
    "For example, when generating a text about **New York**, applying a 2-gram penalty would prevent the model from using the phrase `\"New York\"` more than once.  \n",
    "As a result, the output might sound unnatural or forced, since the model would have to find awkward paraphrases or avoid mentioning the main subject again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c3be7f",
   "metadata": {},
   "source": [
    "`**TODO (Discussion):**` In the following figure, [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751), plot  the probability a model would give to human tect against what beam search does. Based on the plot do you think beam search is a viable solution or not?\n",
    "\n",
    "<center><img src=\"./assets/word_probabilities.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43eddbe",
   "metadata": {},
   "source": [
    "High quality human language does not follow a distribution of high probability next words. In other words, as humans, we want generated text to surprise us and not to be boring/predictable. Hence, beam search cannot be regarded as a viable generation strategy if generating high quality human language is the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74fb916",
   "metadata": {},
   "source": [
    "## Random Sampling\n",
    "\n",
    "Random sampling is a **stochastic decoding strategy**. Instead of always picking the most likely next word (as in greedy or beam search), the model **samples** a word from the probability distribution predicted for the next token.\n",
    "\n",
    "This means that every token has a chance to be selected, **proportional to its probability**.  \n",
    "Words with higher probabilities are more likely to be chosen, but lower-probability words can still appear occasionally.\n",
    "\n",
    "Formally, we can write this as:\n",
    "\n",
    "\\begin{align}\n",
    "w_t ~ P(w |w_{1:t-1})\n",
    "\\end{align}\n",
    "\n",
    "This means that the next word `w_t` is drawn **at random** according to its probability given all previous words `w‚ÇÅ` to `w‚Çú‚Çã‚ÇÅ`.\n",
    "\n",
    "\n",
    "### Example\n",
    "<center><img src=\"./assets/sampling_search.png\"/></center>\n",
    "\n",
    "Starting from **\"The\"**, the model predicts the following next-token probabilities:\n",
    "\n",
    "| Candidate | Probability |\n",
    "|------------|--------------|\n",
    "| \"nice\" | 0.5 |\n",
    "| \"dog\" | 0.4 |\n",
    "| \"car\" | 0.1 |\n",
    "\n",
    "Instead of deterministically choosing `\"nice\"`, random sampling **draws** one token based on these probabilities.  \n",
    "So depending on chance:\n",
    "- `\"nice\"` might be selected 50% of the time,  \n",
    "- `\"dog\"` 40% of the time,  \n",
    "- `\"car\"` 10% of the time.\n",
    "\n",
    "This makes random sampling **non-deterministic** ‚Äî> two runs with the same input may yield different outputs.\n",
    "\n",
    "`**TODO:**` To activate random sampling in `transformers` you must set `do_sample=True` and deactivate Top-K sampling by setting `top_k=0` (more on this later). Do this and create a generation using the same inputs as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d70ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog but what I love about being a dog cat person is being a pet being with people who can treat you. I feel happy to be such a pet person and get to meet so many people. I\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,         # Activate random sampling \n",
    "    top_k=0 ,              # Deactivate Top-k sampling\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059de54",
   "metadata": {},
   "source": [
    "## Sampling with Temperature\n",
    "\n",
    "When we sample from the model‚Äôs probability distribution, the results can sometimes sound **incoherent or random**. \n",
    "The text may drift off-topic or feel unnatural.  \n",
    "This is a common issue with pure random sampling.\n",
    "\n",
    "To make the generated text more **focused** and **coherent**, we can adjust the *temperature* of the distribution.  \n",
    "Temperature controls how sharply or smoothly the model selects tokens from its probability distribution.\n",
    "\n",
    "Formally, we can express this as sampling from a **temperature-scaled distribution**:\n",
    "\n",
    "\\begin{align}\n",
    "w_t ~ P(w |w_{1:t-1})\n",
    "\\end{align}\n",
    "\n",
    "but with the probabilities modified as follows:\n",
    "\n",
    "\\begin{align}\n",
    "P'(w) = softmax\\left ( \\frac{logits}{temperature} \\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "### How Temperature Works\n",
    "\n",
    "- **Low temperature (< 1)** ‚Üí the distribution becomes *sharper*.  \n",
    "  High-probability words become even more likely, while low-probability ones are suppressed.  \n",
    "  ‚Üí The model behaves more deterministically and produces more coherent, focused text.\n",
    "\n",
    "- **High temperature (> 1)** ‚Üí the distribution becomes *flatter*.  \n",
    "  Low-probability words get a higher chance of being selected.  \n",
    "  ‚Üí The model becomes more creative, but also more prone to incoherence.\n",
    "\n",
    "At `temperature = 1`, sampling behaves as usual ‚Äî no scaling is applied.\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "In the previous example, the model predicted the following next-token probabilities after `\"The\"`:\n",
    "\n",
    "| Candidate | Probability (T=1.0) |\n",
    "|------------|----------------------|\n",
    "| \"nice\" | 0.5 |\n",
    "| \"dog\" | 0.4 |\n",
    "| \"car\" | 0.1 |\n",
    "\n",
    "When we **lower the temperature to 0.6**, the distribution becomes **sharper** ‚Äî  \n",
    "high-probability words like `\"nice\"` dominate, while `\"dog\"` and `\"car\"` are much less likely to be chosen.\n",
    "\n",
    "\n",
    "\n",
    "So, by ‚Äúcooling down‚Äù the sampling, we make the text **more coherent and natural**, at the cost of reducing diversity.\n",
    "\n",
    "`**TODO:**` Create a generration using sampling with temperature by setting the  `temperature` argument to `0.6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0047d198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog but I also love the fact that my cat is not a dog. She is a good, loving dog. I do not like to be held back by other dogs but I think that I have to\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=0,\n",
    "    temperature=0.6,      # Activating temperature scaling with temperature=0.6\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22989287",
   "metadata": {},
   "source": [
    "## Top-k Sampling\n",
    "\n",
    "Pure random sampling considers the *entire* vocabulary when picking the next token.\n",
    "As a result, even low-probability, nonsensical words can occasionally be selected.  \n",
    "This can lead to incoherent or off-topic text.\n",
    "\n",
    "**Top-k sampling** (introduced by Fan et al., 2018) solves this problem by **restricting** the model‚Äôs choices.  \n",
    "Instead of sampling from all possible words, it only samples from the **k most probable** next tokens.\n",
    "\n",
    "Formally, we still sample:\n",
    "\n",
    "\\begin{align}\n",
    "w_t ~ P(w |w_{1:t-1})\n",
    "\\end{align}\n",
    "\n",
    "but after truncating the distribution to its **top k** candidates. All other words are assigned a probability of zero and the remaining ones are renormalized to sum to 1.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. The model predicts a probability distribution for the next token.  \n",
    "2. We keep only the **k most likely words** (e.g., the top 50).  \n",
    "3. The probabilities of these top-k words are **renormalized**.  \n",
    "4. A random token is then sampled from this restricted subset.\n",
    "\n",
    "This ensures that only **plausible words** are considered, while still maintaining some **randomness** and **diversity**.\n",
    "\n",
    "\n",
    "### Example\n",
    "<center><img width=1000 src=\"./assets/top_k_sampling.png\"/></center>\n",
    "\n",
    "Suppose the model predicts the following probabilities after `\"The\"`:\n",
    "\n",
    "| Candidate | Probability |\n",
    "|------------|--------------|\n",
    "| \"nice\" | 0.35 |\n",
    "| \"dog\" | 0.25 |\n",
    "| \"car\" | 0.20 |\n",
    "| \"quantum\" | 0.12 |\n",
    "| \"wibble\" | 0.08 |\n",
    "\n",
    "If we set **k = 3**, we keep only the top-3 candidates:\n",
    "\n",
    "`\"nice\"`, `\"dog\"`, `\"car\"`\n",
    "\n",
    "The remaining words (like `\"quantum\"` and `\"wibble\"`) are ignored completely.  \n",
    "Then, the model samples randomly **only** among the top 3 options.\n",
    "\n",
    "This reduces the chance of producing incoherent or irrelevant tokens  \n",
    "while still allowing variation between `\"nice\"`, `\"dog\"`, and `\"car\"`.\n",
    "\n",
    "`**TODO:**` Create a generration using Top-k sampling by setting the  `top_k` argument to `50`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb3e7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog but what I love about being a dog is I see a beautiful pet being cared for ‚Äì I love having the opportunity to see her every day so I feel very privileged to have been able to help this\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50            # Activate Top-k sampling with k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224f221",
   "metadata": {},
   "source": [
    "`**TODO (Discussion):**` What are your takeaways on Top-K sampling? Can you see any limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655db6e3",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "Top-k sampling improves random sampling by limiting the model‚Äôs choices to the most probable tokens: \n",
    "- Helps prevent incoherent or nonsensical generations.\n",
    "- Keeps a good balance between diversity and fluency.\n",
    "- Works best when combined with temperature scaling for finer control.\n",
    "\n",
    "In short:<br>\n",
    "‚Üí Pure sampling = too random. <br>\n",
    "‚Üí Top-k sampling = controlled creativity.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "While Top-k sampling improves coherence compared to pure sampling, it has one key limitation:  \n",
    "it uses a **fixed number (k)** of candidate words at every step, regardless of how the probability distribution looks.\n",
    "\n",
    "This means:\n",
    "- For **sharp distributions**, Top-k might still include **unlikely or irrelevant words**, which can lead to gibberish.  \n",
    "- For **flat distributions**, it might **exclude reasonable options**, limiting diversity and creativity.\n",
    "\n",
    "Because it doesn‚Äôt adapt dynamically to the shape of the distribution `P(w | w‚ÇÅ:‚Çú‚Çã‚ÇÅ)`,  \n",
    "Top-k sampling can sometimes be **too restrictive** or **too random** depending on context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725acb08",
   "metadata": {},
   "source": [
    "## Top-p (Nucleus) Sampling\n",
    "\n",
    "**Top-p sampling** (also known as **nucleus sampling**) was introduced as an improvement over Top-k sampling.\n",
    "\n",
    "Instead of keeping a **fixed number (k)** of the most probable words at each step, Top-p sampling dynamically chooses the **smallest possible set of words** whose **cumulative** probability exceeds a threshold *p* (for example, 0.9).  \n",
    "\n",
    "Formally, we still sample:\n",
    "\n",
    "\\begin{align}\n",
    "w_t ~ P(w | w‚ÇÅ:‚Çú‚Çã‚ÇÅ)\n",
    "\\end{align}\n",
    "\n",
    "but we truncate the distribution to include only the top words such that:\n",
    "\\begin{align}\n",
    "\\sum P(w) ‚â• p\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "and renormalize their probabilities before sampling.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. The model predicts a probability distribution for the next token.  \n",
    "2. Sort tokens by probability (highest ‚Üí lowest).  \n",
    "3. Keep the **smallest subset** of tokens whose cumulative probability ‚â• *p*.  \n",
    "4. Sample randomly from this subset.\n",
    "\n",
    "This allows the model to **adapt dynamically**:  \n",
    "- If the distribution is *sharp*, only a few tokens are considered.  \n",
    "- If the distribution is *flat*, more tokens are included.  \n",
    "\n",
    "The result is a flexible trade-off between **coherence** and **diversity**.\n",
    "\n",
    "\n",
    "### Example\n",
    "<center><img width=1000 src=\"./assets/top_p_sampling.png\"/></center>\n",
    "\n",
    "Assume the model predicts these next-token probabilities after `\"The\"`:\n",
    "\n",
    "| Candidate | Probability | Cumulative |\n",
    "|------------|--------------|-------------|\n",
    "| \"nice\" | 0.5 | 0.5 |\n",
    "| \"dog\" | 0.3 | 0.8 |\n",
    "| \"car\" | 0.1 | 0.9 |\n",
    "| \"quantum\" | 0.05 | 0.95 |\n",
    "| \"wibble\" | 0.05 | 1.00 |\n",
    "\n",
    "If we set **p = 0.9**, we include tokens until the cumulative probability reaches 0.9:  \n",
    "‚Üí `\"nice\"`, `\"car\"`, and `\"dog\"` are kept; the rest are discarded.  \n",
    "\n",
    "Then the model samples **only** from those top-p words, ensuring that the sample space adapts to the probability distribution‚Äôs shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13a8c856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog but what I love about being a dog cat person is being a pet being with people who can treat you. I feel happy to be such a pet person and get to meet so many people. I\n"
     ]
    }
   ],
   "source": [
    "\n",
    "set_seed(42)\n",
    "\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,          # Activative nucleus sampling with p=0.92\n",
    "    top_k=0              # Deactivate Top-k sampling\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
