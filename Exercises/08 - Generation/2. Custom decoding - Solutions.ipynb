{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931b3d61",
   "metadata": {},
   "source": [
    "# üß† Building Decoding Strategies from Scratch\n",
    "\n",
    "### üéØ Objective\n",
    "In this session, we‚Äôll **recreate common decoding strategies step by step**, using only the model‚Äôs raw output probabilities (logits).  \n",
    "You‚Äôve already seen how Hugging Face‚Äôs `generate()` method can perform greedy search, beam search, top-k, and nucleus (top-p) sampling for you.  \n",
    "Now it‚Äôs time to **open the black box** and see *how those algorithms actually work under the hood.*\n",
    "\n",
    "\n",
    "### üß© What You‚Äôll Learn\n",
    "By the end of this notebook, you‚Äôll be able to:\n",
    "- Access token probabilities from a language model (e.g., GPT-2).  \n",
    "- Implement your own:\n",
    "  - **Greedy Search**\n",
    "  - **Beam Search**\n",
    "  - **Top-k Sampling**\n",
    "  - **Nucleus (Top-p) Sampling**\n",
    "- Compare their behavior and outputs across different prompts.  \n",
    "- Understand the trade-offs between determinism, diversity, and coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd65980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f7678",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup and First Generation\n",
    "\n",
    "Before we start building decoding strategies manually, let‚Äôs load a pretrained language model and generate some text using the **built-in Hugging Face API**.  \n",
    "This will serve as our **baseline** ‚Äî we‚Äôll soon replace this simple `.generate()` call with our own decoding logic.\n",
    "\n",
    "\n",
    "`**TODO:**`\n",
    "\n",
    "1. **Import dependencies** ‚Äî we‚Äôll use `transformers` for the model and tokenizer, and `torch` for tensor operations.  \n",
    "2. **Load GPT-2** ‚Äî a small, autoregressive model trained to predict the next token in a sequence.  \n",
    "3. **Prepare the input** ‚Äî we‚Äôll encode a short prompt into token IDs.  \n",
    "4. **Generate text** ‚Äî using the model‚Äôs default decoding (greedy search).  \n",
    "5. **Decode the output** ‚Äî back to human-readable text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397cd945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: I have a dream of being a doctor.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "text = \"I have a dream\"\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=len(input_ids.squeeze())+5)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Generated text: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed66f1a6",
   "metadata": {},
   "source": [
    "## üöÄ Implementing Greedy Search\n",
    "\n",
    "### üß† Concept\n",
    "\n",
    "Greedy Search is the most straightforward decoding method:\n",
    "- At each step, the model predicts a probability distribution over the vocabulary.\n",
    "- We pick **only the most probable token** (the one with the highest logit or probability).\n",
    "- That token is appended to the sequence and fed back into the model.\n",
    "- The process repeats until we reach the desired length or an end-of-sentence token.\n",
    "\n",
    "This strategy is **deterministic** and **fast**, but often leads to repetitive or locally optimal results ‚Äî the model never explores alternative continuations.\n",
    "\n",
    "### ‚öôÔ∏è Implementation Hints\n",
    "\n",
    "In this method:\n",
    "1. We run the model on the current sequence to obtain the logits.\n",
    "2. We take the **argmax** over the last-token logits to choose the next token.\n",
    "3. We append that token to the input sequence.\n",
    "4. We recursively continue until we‚Äôve generated the target number of tokens.\n",
    "\n",
    "**`TODO:`** Implement the `greedy_search` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a89115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: I have a dream of being a doctor.\n"
     ]
    }
   ],
   "source": [
    "def greedy_search(\n",
    "    input_ids: torch.Tensor,   # Current sequence of token IDs (shape: [1, seq_len])\n",
    "    length: int = 5            # Number of tokens left to generate\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs recursive Greedy Search decoding.\n",
    "\n",
    "    At each step, the model selects the most probable next token\n",
    "    (the one with the highest logit value) and appends it to the sequence.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Current sequence of token IDs (1 x seq_len tensor).\n",
    "        length: Number of tokens left to generate.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing the full generated token sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    if length == 0:\n",
    "        return input_ids\n",
    "\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "    # Get the predicted next sub-word (here we use top-k search)\n",
    "    logits = predictions[0, -1, :]\n",
    "    token_id = torch.argmax(logits).unsqueeze(0)\n",
    "\n",
    "    # Add the predicted token to the list of input ids\n",
    "    new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0)], dim=-1)\n",
    "\n",
    "    # Recursive call\n",
    "    input_ids = greedy_search(new_input_ids, length-1)\n",
    "    \n",
    "    return input_ids\n",
    "\n",
    "# Start generating text\n",
    "output_ids = greedy_search(input_ids, length=5)\n",
    "output = tokenizer.decode(output_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "print(f\"Generated text: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81363d17",
   "metadata": {},
   "source": [
    "## üßÆ Helper Function: `get_log_prob`\n",
    "\n",
    "This small utility computes the **log-probability** of a chosen token given the model‚Äôs output logits.  \n",
    "It‚Äôs useful for tracking and comparing sequence scores across decoding strategies.\n",
    "\n",
    "**How it works:**\n",
    "1. Applies a softmax to convert logits into probabilities.  \n",
    "2. Takes the logarithm of those probabilities.  \n",
    "3. Returns the log-probability corresponding to the selected `token_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aaa7a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob(logits, token_id):\n",
    "    # Compute the softmax of the logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    log_probabilities = torch.log(probabilities)\n",
    "    \n",
    "    # Get the log probability of the token\n",
    "    token_log_probability = log_probabilities[token_id].item()\n",
    "\n",
    "    return token_log_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a85571",
   "metadata": {},
   "source": [
    "## üöÄ Implementing Beam Search\n",
    "\n",
    "### üß† Concept\n",
    "\n",
    "Beam Search improves upon Greedy Search by keeping track of **multiple candidate sequences** (called *beams*) instead of just one.  \n",
    "At each generation step, instead of picking only the single most likely token, we explore the **top-k next tokens** for each current sequence (where *k* is the beam width).  \n",
    "We then keep only the *k* best overall sequences based on their **cumulative log-probability scores**.\n",
    "\n",
    "This strategy balances **exploration** and **exploitation** ‚Äî it often produces more coherent text than Greedy Search, though it‚Äôs more computationally expensive.\n",
    "\n",
    "### ‚öôÔ∏è Implementation Hints\n",
    "\n",
    "In this implementation:\n",
    "1. For each call, we compute the model‚Äôs output logits for the current sequence.  \n",
    "2. We extract the **top-k tokens** (beam width) with the highest logit values.  \n",
    "3. For each of these tokens:\n",
    "   - Compute its log-probability.  \n",
    "   - Append it to the input sequence.  \n",
    "   - Recursively continue the search for the remaining steps.  \n",
    "4. Once all beams reach the target length, we return all completed sequences with their total scores and pick the **best one**.\n",
    "\n",
    "**`TODO:`** Implement the `beam_search` function below.  \n",
    "Focus on:\n",
    "- Expanding each beam with the top-k tokens.  \n",
    "- Keeping track of cumulative scores.  \n",
    "- Returning all completed sequences so you can later select the best one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0866f8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Generated text: I have a dream. I have a dream\n"
     ]
    }
   ],
   "source": [
    "def beam_search(\n",
    "    input_ids: torch.Tensor,       # Current input token IDs (shape: [1, seq_len])\n",
    "    length: int,                   # Number of tokens left to generate\n",
    "    beams: int,                    # Beam width (number of candidate sequences to keep)\n",
    "    score: Optional[float] = None  # Cumulative log-probability of the sequence so far\n",
    ") -> List[Dict[str, torch.Tensor]]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs recursive Beam Search decoding.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Current sequence of token IDs (1 x seq_len tensor).\n",
    "        length: Number of tokens left to generate.\n",
    "        beams: Number of top candidate tokens to expand at each step.\n",
    "        score: Optional cumulative log-probability for the sequence so far.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, each containing:\n",
    "            - \"new_input_ids\": the generated token sequence (tensor)\n",
    "            - \"score\": the cumulative log-probability score (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    if length == 0:\n",
    "        return [{\"new_input_ids\": input_ids, \"score\": score}]\n",
    "\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "    # Select the top-k most probable next tokens (beam expansion)\n",
    "    logits = predictions[0, -1, :]\n",
    "\n",
    "    top_token_ids = torch.topk(logits, beams).indices\n",
    "\n",
    "    outputs = []\n",
    "    for j, token_id in enumerate(top_token_ids):\n",
    "\n",
    "        # Compute the score of the predicted token\n",
    "        token_score = get_log_prob(logits, token_id)\n",
    "        cumulative_score = token_score + (score if score is not None else 0.0)\n",
    "\n",
    "        # Add the predicted token to the list of input ids\n",
    "        new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "\n",
    "        # Recursive call\n",
    "        outs = beam_search(new_input_ids, length-1, beams, cumulative_score)\n",
    "        outputs.extend(outs)\n",
    "    return outputs\n",
    "\n",
    "# Start generating text\n",
    "output_ids = beam_search(input_ids, length=5, beams=2)\n",
    "best_entry = max(output_ids, key=lambda x: x[\"score\"])\n",
    "best_output = tokenizer.decode(best_entry[\"new_input_ids\"].squeeze().tolist(), skip_special_tokens=True)\n",
    "print(f\"Best Generated text: {best_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a19f1",
   "metadata": {},
   "source": [
    "## üöÄ Implementing Top-k Sampling from Scratch\n",
    "\n",
    "### üß† Concept\n",
    "\n",
    "So far, **Greedy Search** and **Beam Search** always choose the most likely tokens ‚Äî making them deterministic but sometimes repetitive or predictable.  \n",
    "To introduce more **creativity and diversity**, we can use **sampling-based decoding** methods.\n",
    "\n",
    "**Top-k Sampling** limits randomness to a controlled subset of possible next tokens:\n",
    "1. At each step, we look at the model‚Äôs logits for all tokens.\n",
    "2. We keep only the **top-k most probable tokens**.\n",
    "3. We **mask out** all other tokens by setting their logits to `-inf`.\n",
    "4. We apply a softmax over the remaining tokens to obtain a proper probability distribution.\n",
    "5. We **randomly sample** one token from this smaller set.\n",
    "6. Append the chosen token and continue.\n",
    "\n",
    "This way, we don‚Äôt always pick the top token (like in greedy search), but we avoid sampling from the long tail of improbable words ‚Äî a balance between **coherence** and **diversity**.\n",
    "\n",
    "### ‚öôÔ∏è Implementation Hints\n",
    "\n",
    "In this method:\n",
    "- Use `torch.topk(logits, top_k)` to find the cutoff threshold.  \n",
    "- Set all logits below that threshold to `-inf` (so their probability becomes zero after softmax).  \n",
    "- Use `torch.multinomial()` to sample one token ID according to the resulting probability distribution.  \n",
    "- Recursively repeat the process for the desired number of tokens.  \n",
    "- Remember to use a **manual seed** (e.g., `torch.manual_seed(0)`) for reproducibility in experiments.\n",
    "\n",
    "\n",
    "**`TODO:`** Implement the `top_k_sampling` function below.  \n",
    "Try different `top_k` values (e.g., 5, 20, 100) and observe how the output‚Äôs **creativity** changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d22c0be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: I have a dream,\" she said, referring\n"
     ]
    }
   ],
   "source": [
    "def top_k_sampling(\n",
    "    input_ids: torch.Tensor,   # Current sequence of token IDs (shape: [1, seq_len])\n",
    "    length: int,               # Number of tokens left to generate\n",
    "    top_k: int                 # Number of top tokens to sample from at each step\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs recursive Top-k Sampling decoding.\n",
    "\n",
    "    At each step:\n",
    "    1. We take the model‚Äôs logits for the next token.\n",
    "    2. We keep only the top-k most probable tokens.\n",
    "    3. We apply softmax to get a probability distribution.\n",
    "    4. We sample one token from that reduced set (introducing controlled randomness).\n",
    "    5. Append it to the sequence and continue recursively.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Current sequence of token IDs (1 x seq_len tensor).\n",
    "        length: Number of tokens left to generate.\n",
    "        top_k: Number of highest-probability tokens to consider at each step.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing the full generated token sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    if length == 0:\n",
    "        return input_ids\n",
    "\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "    # Get the predicted next sub-word (here we use top-k search)\n",
    "    logits = predictions[0, -1, :]\n",
    "\n",
    "    assert top_k >= 1\n",
    "\n",
    "    # Setting the manual seed for reproducibility. This is done in a simplistic way here for demonstration purposes.\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "    new_logits = torch.clone(logits)\n",
    "    new_logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(new_logits, dim=-1)\n",
    "\n",
    "    # Sample n tokens from the resulting distribution\n",
    "    top_token_id = torch.multinomial(probabilities, 1)[0]\n",
    "    \n",
    "    new_input_ids = torch.cat([input_ids, top_token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "\n",
    "    out = top_k_sampling(new_input_ids, length-1, top_k)\n",
    "    return out\n",
    "\n",
    "output_ids = top_k_sampling(input_ids, length=5, top_k=20)\n",
    "output = tokenizer.decode(output_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "print(f\"Generated text: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4a495",
   "metadata": {},
   "source": [
    "## üå°Ô∏è Adding Temperature to Top-k Sampling\n",
    "\n",
    "### üß† Concept\n",
    "**Temperature** controls how ‚Äúconfident‚Äù or ‚Äúcreative‚Äù the model‚Äôs sampling behavior is.  \n",
    "Before applying softmax, we divide the logits by a temperature value:\n",
    "\n",
    "\\begin{align}\n",
    "p_i = \\text{softmax}\\left(\\frac{\\text{logits}_i}{T}\\right)\n",
    "\\end{align}\n",
    "\n",
    "- **Low temperature (< 1)** ‚Üí sharper distribution ‚Üí more deterministic, focused outputs  \n",
    "- **High temperature (> 1)** ‚Üí flatter distribution ‚Üí more random, diverse outputs  \n",
    "\n",
    "**`TODO:`**  \n",
    "Modify the Top-k Sampling implementation so that logits are divided by `temperature` **before** applying softmax.  \n",
    "Then, experiment with different temperature values (e.g., `0.7`, `1.0`, `1.5`) and observe how the output‚Äôs creativity changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72c46297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: I have a dream of becoming one,\" she\n"
     ]
    }
   ],
   "source": [
    "def top_k_sampling(\n",
    "    input_ids: torch.Tensor,   # Current sequence of token IDs (shape: [1, seq_len])\n",
    "    length: int,               # Number of tokens left to generate\n",
    "    top_k: int,                # Number of top tokens to sample from at each step\n",
    "    temperature: float = 1.0   # Temperature parameter controlling randomness\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs recursive Top-k Sampling with Temperature.\n",
    "\n",
    "    This decoding method introduces *controlled randomness*:\n",
    "    - Restricts sampling to the top-k most probable tokens.\n",
    "    - Scales the logits by a temperature before applying softmax.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Current sequence of token IDs (1 x seq_len tensor).\n",
    "        length: Number of tokens left to generate.\n",
    "        top_k: Number of highest-probability tokens to consider at each step.\n",
    "        temperature: Value > 0 that controls randomness.\n",
    "            ‚Ä¢ Lower (<1) ‚Üí sharper distribution, more deterministic.\n",
    "            ‚Ä¢ Higher (>1) ‚Üí flatter distribution, more random.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing the full generated token sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    if length == 0:\n",
    "        return input_ids\n",
    "\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "    # Get the predicted next sub-word (here we use top-k search)\n",
    "    logits = predictions[0, -1, :]\n",
    "\n",
    "    assert top_k >= 1\n",
    "\n",
    "    # Setting the manual seed for reproducibility. This is done in a simplistic way here for demonstration purposes.\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "    new_logits = torch.clone(logits)\n",
    "    new_logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(new_logits / temperature, dim=-1)\n",
    "\n",
    "    # Sample n tokens from the resulting distribution\n",
    "    top_token_id = torch.multinomial(probabilities, 1)[0]\n",
    "\n",
    "    \n",
    "    token_score = get_log_prob(logits, top_token_id)\n",
    "    new_input_ids = torch.cat([input_ids, top_token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "\n",
    "    out = top_k_sampling(new_input_ids, length-1, top_k, 1)\n",
    "    return out\n",
    "\n",
    "output_ids = top_k_sampling(input_ids, length=5, top_k=20, temperature=0.2)\n",
    "output = tokenizer.decode(output_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "print(f\"Generated text: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
