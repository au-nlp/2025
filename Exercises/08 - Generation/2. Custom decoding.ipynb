{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931b3d61",
   "metadata": {},
   "source": [
    "# 🧠 Building Decoding Strategies from Scratch\n",
    "\n",
    "### 🎯 Objective\n",
    "In this session, we’ll **recreate common decoding strategies step by step**, using only the model’s raw output probabilities (logits).  \n",
    "You’ve already seen how Hugging Face’s `generate()` method can perform greedy search, beam search, top-k, and nucleus (top-p) sampling for you.  \n",
    "Now it’s time to **open the black box** and see *how those algorithms actually work under the hood.*\n",
    "\n",
    "\n",
    "### 🧩 What You’ll Learn\n",
    "By the end of this notebook, you’ll be able to:\n",
    "- Access token probabilities from a language model (e.g., GPT-2).  \n",
    "- Implement your own:\n",
    "  - **Greedy Search**\n",
    "  - **Beam Search**\n",
    "  - **Top-k Sampling**\n",
    "  - **Nucleus (Top-p) Sampling**\n",
    "- Compare their behavior and outputs across different prompts.  \n",
    "- Understand the trade-offs between determinism, diversity, and coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd65980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f7678",
   "metadata": {},
   "source": [
    "## ⚙️ Setup and First Generation\n",
    "\n",
    "Before we start building decoding strategies manually, let’s load a pretrained language model and generate some text using the **built-in Hugging Face API**.  \n",
    "This will serve as our **baseline** — we’ll soon replace this simple `.generate()` call with our own decoding logic.\n",
    "\n",
    "\n",
    "`**TODO:**`\n",
    "\n",
    "1. **Import dependencies** — we’ll use `transformers` for the model and tokenizer, and `torch` for tensor operations.  \n",
    "2. **Load GPT-2** — a small, autoregressive model trained to predict the next token in a sequence.  \n",
    "3. **Prepare the input** — we’ll encode a short prompt into token IDs.  \n",
    "4. **Generate text** — using the model’s default decoding (greedy search).  \n",
    "5. **Decode the output** — back to human-readable text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "text = \"I have a dream\"\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=len(input_ids.squeeze())+5)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Generated text: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed66f1a6",
   "metadata": {},
   "source": [
    "## 🚀 Implementing Greedy Search\n",
    "\n",
    "### 🧠 Concept\n",
    "\n",
    "Greedy Search is the most straightforward decoding method:\n",
    "- At each step, the model predicts a probability distribution over the vocabulary.\n",
    "- We pick **only the most probable token** (the one with the highest logit or probability).\n",
    "- That token is appended to the sequence and fed back into the model.\n",
    "- The process repeats until we reach the desired length or an end-of-sentence token.\n",
    "\n",
    "This strategy is **deterministic** and **fast**, but often leads to repetitive or locally optimal results — the model never explores alternative continuations.\n",
    "\n",
    "### ⚙️ Implementation Hints\n",
    "\n",
    "In this method:\n",
    "1. We run the model on the current sequence to obtain the logits.\n",
    "2. We take the **argmax** over the last-token logits to choose the next token.\n",
    "3. We append that token to the input sequence.\n",
    "4. We recursively continue until we’ve generated the target number of tokens.\n",
    "\n",
    "**`TODO:`** Implement the `greedy_search` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a89115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(\n",
    "    input_ids: torch.Tensor,   # Current sequence of token IDs (shape: [1, seq_len])\n",
    "    length: int = 5            # Number of tokens left to generate\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs recursive Greedy Search decoding.\n",
    "\n",
    "    At each step, the model selects the most probable next token\n",
    "    (the one with the highest logit value) and appends it to the sequence.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Current sequence of token IDs (1 x seq_len tensor).\n",
    "        length: Number of tokens left to generate.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing the full generated token sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Greedy Search decoding\n",
    "\n",
    "# Start generating text\n",
    "output_ids = greedy_search(input_ids, length=5)\n",
    "output = tokenizer.decode(output_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "print(f\"Generated text: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81363d17",
   "metadata": {},
   "source": [
    "## 🧮 Helper Function: `get_log_prob`\n",
    "\n",
    "This small utility computes the **log-probability** of a chosen token given the model’s output logits.  \n",
    "It’s useful for tracking and comparing sequence scores across decoding strategies.\n",
    "\n",
    "**How it works:**\n",
    "1. Applies a softmax to convert logits into probabilities.  \n",
    "2. Takes the logarithm of those probabilities.  \n",
    "3. Returns the log-probability corresponding to the selected `token_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa7a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob(logits, token_id):\n",
    "    # Compute the softmax of the logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    log_probabilities = torch.log(probabilities)\n",
    "    \n",
    "    # Get the log probability of the token\n",
    "    token_log_probability = log_probabilities[token_id].item()\n",
    "\n",
    "    return token_log_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a85571",
   "metadata": {},
   "source": [
    "## 🚀 Implementing Beam Search\n",
    "\n",
    "### 🧠 Concept\n",
    "\n",
    "Beam Search improves upon Greedy Search by keeping track of **multiple candidate sequences** (called *beams*) instead of just one.  \n",
    "At each generation step, instead of picking only the single most likely token, we explore the **top-k next tokens** for each current sequence (where *k* is the beam width).  \n",
    "We then keep only the *k* best overall sequences based on their **cumulative log-probability scores**.\n",
    "\n",
    "This strategy balances **exploration** and **exploitation** — it often produces more coherent text than Greedy Search, though it’s more computationally expensive.\n",
    "\n",
    "### ⚙️ Implementation Hints\n",
    "\n",
    "In this implementation:\n",
    "1. For each call, we compute the model’s output logits for the current sequence.  \n",
    "2. We extract the **top-k tokens** (beam width) with the highest logit values.  \n",
    "3. For each of these tokens:\n",
    "   - Compute its log-probability.  \n",
    "   - Append it to the input sequence.  \n",
    "   - Recursively continue the search for the remaining steps.  \n",
    "4. Once all beams reach the target length, we return all completed sequences with their total scores and pick the **best one**.\n",
    "\n",
    "**`TODO:`** Implement the `beam_search` function below.  \n",
    "Focus on:\n",
    "- Expanding each beam with the top-k tokens.  \n",
    "- Keeping track of cumulative scores.  \n",
    "- Returning all completed sequences so you can later select the best one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(\n",
    "    input_ids: torch.Tensor,       # Current input token IDs (shape: [1, seq_len])\n",
    "    length: int,                   # Number of tokens left to generate\n",
    "    beams: int,                    # Beam width (number of candidate sequences to keep)\n",
    "    score: Optional[float] = None  # Cumulative log-probability of the sequence so far\n",
    ") -> List[Dict[str, torch.Tensor]]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs recursive Beam Search decoding.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Current sequence of token IDs (1 x seq_len tensor).\n",
    "        length: Number of tokens left to generate.\n",
    "        beams: Number of top candidate tokens to expand at each step.\n",
    "        score: Optional cumulative log-probability for the sequence so far.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, each containing:\n",
    "            - \"new_input_ids\": the generated token sequence (tensor)\n",
    "            - \"score\": the cumulative log-probability score (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Beam Search decoding\n",
    "\n",
    "# Start generating text\n",
    "output_ids = beam_search(input_ids, length=5, beams=2)\n",
    "best_entry = max(output_ids, key=lambda x: x[\"score\"])\n",
    "best_output = tokenizer.decode(best_entry[\"new_input_ids\"].squeeze().tolist(), skip_special_tokens=True)\n",
    "print(f\"Best Generated text: {best_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a19f1",
   "metadata": {},
   "source": [
    "## 🚀 Implementing Top-k Sampling from Scratch\n",
    "\n",
    "### 🧠 Concept\n",
    "\n",
    "So far, **Greedy Search** and **Beam Search** always choose the most likely tokens — making them deterministic but sometimes repetitive or predictable.  \n",
    "To introduce more **creativity and diversity**, we can use **sampling-based decoding** methods.\n",
    "\n",
    "**Top-k Sampling** limits randomness to a controlled subset of possible next tokens:\n",
    "1. At each step, we look at the model’s logits for all tokens.\n",
    "2. We keep only the **top-k most probable tokens**.\n",
    "3. We **mask out** all other tokens by setting their logits to `-inf`.\n",
    "4. We apply a softmax over the remaining tokens to obtain a proper probability distribution.\n",
    "5. We **randomly sample** one token from this smaller set.\n",
    "6. Append the chosen token and continue.\n",
    "\n",
    "This way, we don’t always pick the top token (like in greedy search), but we avoid sampling from the long tail of improbable words — a balance between **coherence** and **diversity**.\n",
    "\n",
    "### ⚙️ Implementation Hints\n",
    "\n",
    "In this method:\n",
    "- Use `torch.topk(logits, top_k)` to find the cutoff threshold.  \n",
    "- Set all logits below that threshold to `-inf` (so their probability becomes zero after softmax).  \n",
    "- Use `torch.multinomial()` to sample one token ID according to the resulting probability distribution.  \n",
    "- Recursively repeat the process for the desired number of tokens.  \n",
    "- Remember to use a **manual seed** (e.g., `torch.manual_seed(0)`) for reproducibility in experiments.\n",
    "\n",
    "\n",
    "**`TODO:`** Implement the `top_k_sampling` function below.  \n",
    "Try different `top_k` values (e.g., 5, 20, 100) and observe how the output’s **creativity** changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c0be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sampling(\n",
    "    input_ids: torch.Tensor,   # Current sequence of token IDs (shape: [1, seq_len])\n",
    "    length: int,               # Number of tokens left to generate\n",
    "    top_k: int                 # Number of top tokens to sample from at each step\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs recursive Top-k Sampling decoding.\n",
    "\n",
    "    At each step:\n",
    "    1. We take the model’s logits for the next token.\n",
    "    2. We keep only the top-k most probable tokens.\n",
    "    3. We apply softmax to get a probability distribution.\n",
    "    4. We sample one token from that reduced set (introducing controlled randomness).\n",
    "    5. Append it to the sequence and continue recursively.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Current sequence of token IDs (1 x seq_len tensor).\n",
    "        length: Number of tokens left to generate.\n",
    "        top_k: Number of highest-probability tokens to consider at each step.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing the full generated token sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Top-k Sampling decoding\n",
    "\n",
    "output_ids = top_k_sampling(input_ids, length=5, top_k=20)\n",
    "output = tokenizer.decode(output_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "print(f\"Generated text: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4a495",
   "metadata": {},
   "source": [
    "## 🌡️ Adding Temperature to Top-k Sampling\n",
    "\n",
    "### 🧠 Concept\n",
    "**Temperature** controls how “confident” or “creative” the model’s sampling behavior is.  \n",
    "Before applying softmax, we divide the logits by a temperature value:\n",
    "\n",
    "\\begin{align}\n",
    "p_i = \\text{softmax}\\left(\\frac{\\text{logits}_i}{T}\\right)\n",
    "\\end{align}\n",
    "\n",
    "- **Low temperature (< 1)** → sharper distribution → more deterministic, focused outputs  \n",
    "- **High temperature (> 1)** → flatter distribution → more random, diverse outputs  \n",
    "\n",
    "**`TODO:`**  \n",
    "Modify the Top-k Sampling implementation so that logits are divided by `temperature` **before** applying softmax.  \n",
    "Then, experiment with different temperature values (e.g., `0.7`, `1.0`, `1.5`) and observe how the output’s creativity changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c46297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sampling(\n",
    "    input_ids: torch.Tensor,   # Current sequence of token IDs (shape: [1, seq_len])\n",
    "    length: int,               # Number of tokens left to generate\n",
    "    top_k: int,                # Number of top tokens to sample from at each step\n",
    "    temperature: float = 1.0   # Temperature parameter controlling randomness\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs recursive Top-k Sampling with Temperature.\n",
    "\n",
    "    This decoding method introduces *controlled randomness*:\n",
    "    - Restricts sampling to the top-k most probable tokens.\n",
    "    - Scales the logits by a temperature before applying softmax.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Current sequence of token IDs (1 x seq_len tensor).\n",
    "        length: Number of tokens left to generate.\n",
    "        top_k: Number of highest-probability tokens to consider at each step.\n",
    "        temperature: Value > 0 that controls randomness.\n",
    "            • Lower (<1) → sharper distribution, more deterministic.\n",
    "            • Higher (>1) → flatter distribution, more random.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing the full generated token sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Top-k Sampling with Temperature decoding\n",
    "\n",
    "output_ids = top_k_sampling(input_ids, length=5, top_k=20, temperature=0.2)\n",
    "output = tokenizer.decode(output_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "print(f\"Generated text: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
