{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80528924",
   "metadata": {
    "id": "80528924"
   },
   "source": [
    "# Part1： MCQs (3 Points * 10 = 30 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781b850d",
   "metadata": {},
   "source": [
    "Each question has only **one** correct option, and there are no partial points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4265adee",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "**What is the primary purpose of backpropagation in neural networks?**\n",
    "\n",
    "A. To compute gradients of the loss function with respect to the input data\n",
    "\n",
    "B. To adjust the learning rate dynamically during training\n",
    "\n",
    "C. To propagate error values backward through the network to refine predictions\n",
    "\n",
    "D. To compute gradients of the loss function with respect to the network’s parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d903b4ec",
   "metadata": {},
   "source": [
    "**Your Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eca1ae",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "**A model has a perplexity of 20 on dataset A and 40 on dataset B. Which conclusion is correct?**\n",
    "\n",
    "A. The model performs twice as well on A as on B \n",
    "\n",
    "B. The model is twice as confident on A as on B\n",
    "\n",
    "C. The model’s vocabulary coverage is higher on dataset B than on dataset A. \n",
    "\n",
    "D. Perplexity cannot be compared across datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00438076",
   "metadata": {},
   "source": [
    "**Your Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8025300",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "**Why does RLHF use PPO (Proximal Policy Optimization) instead of standard policy gradient methods like REINFORCE?**\n",
    "\n",
    "A. PPO avoids catastrophic forgetting\n",
    "\n",
    "B. PPO allows the model to explore unseen tokens safely\n",
    "\n",
    "C. PPO reduces the reward variance\n",
    "\n",
    "D. PPO stabilizes updates by limiting policy drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028e801",
   "metadata": {},
   "source": [
    "**Your Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44328156",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Consider the following statements about Word2Vec:\n",
    "\n",
    "**Statements**\n",
    "1. Word2Vec usually can generalize better to rare and unseen words than those using one-hot vectors.  \n",
    "2. Both CBOW and Skip-gram algorithms do not take into account the order of words that appear in the context window.  \n",
    "3. Shuffling the order of words within each sentence does not affect the Word2Vec vector representations.  \n",
    "\n",
    "**Which of the above statements are TRUE?**\n",
    "\n",
    "A. 2  \n",
    "\n",
    "B. 1, 3 \n",
    "\n",
    "C. 1, 2 \n",
    "\n",
    "D. 2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b722c539",
   "metadata": {},
   "source": [
    "**Your Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2599f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d383292",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "**Which of the following accurately describes a key disadvantage of using recurrent neural networks (RNNs) for modeling long sequences?**\n",
    "\n",
    "- A. RNNs compress all historical context into a single fixed-size state vector, causing information loss.  \n",
    "- B. RNNs always have quadratic computational complexity with respect to sequence length, making them impractical for long sequences.  \n",
    "- C. RNNs require extremely large datasets for training because each timestep uses different parameter sets.  \n",
    "- D. RNNs cannot learn nonlinear representations because their hidden states must remain linear combinations of past inputs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb11c0c",
   "metadata": {},
   "source": [
    "**Your Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba28fb",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "**In a Transformer encoder-decoder model, each decoder block utilizes masked self-attention followed by encoder-decoder cross-attention. Which of the following correctly explains the role of masked self-attention in the decoder?**\n",
    "\n",
    "A. It enables the decoder to attend to all encoder outputs, aligning the input and output sequences.  \n",
    "\n",
    "B. It randomly masks part of the encoder outputs to act as a regularization mechanism similar to dropout, preventing overfitting. \n",
    "\n",
    "C. It allows the decoder to attend to all positions in the output sequence during training to improve parallelism.\n",
    "\n",
    "D. It ensures the decoder does not attend to future tokens during training to maintain autoregressive property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745becae",
   "metadata": {},
   "source": [
    "**Your Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aee8b9",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Given a training set containing the following sequences:\n",
    "\n",
    "> \"the cat sat on the mat\"  \n",
    "> \"the dog chased after the cat\"  \n",
    "> \"a cat and a dog ran\"  \n",
    "> \"she sat on the sofa\"  \n",
    "> \"they bought a new mat\"  \n",
    "> \"the sofa was comfortable\"\n",
    "\n",
    "The training set includes **18 unique tokens** and **32 tokens in total**.  We consider the words split by whitespace as the tokens in the vocabulary. We don’t consider adding any special tokens (e.g., `<UNK>` or `<stop>`) unless specifically stated.  \n",
    "\n",
    "Please answer the following questions in the context of **N-gram language modeling**:\n",
    "\n",
    "To avoid the Out of Vocabulary problem, we apply the add-one Laplace with smoothing, where the probability of each bigram can be computed as:\n",
    "\n",
    "$P(A|B) = \\frac{\\text{count}(B, A) + \\alpha}{\\text{count}(B) + \\alpha \\cdot |V|}$\n",
    "\n",
    "After applying add-one Laplace smoothing $\\alpha = 1$ to the bigram model, what are the probabilities of $P(\\text{sat}|\\text{on})$ and $P(\\text{on}|\\text{sat})$?\n",
    "\n",
    "---\n",
    "\n",
    "A. None of the other options is correct.  \n",
    "\n",
    "B. $P(\\text{sat}|\\text{on}) = \\frac{1}{20}$, $P(\\text{on}|\\text{sat}) = \\frac{3}{20}$.  \n",
    "\n",
    "C. $P(\\text{sat}|\\text{on}) = 1$, $P(\\text{on}|\\text{sat}) = 0$.  \n",
    "\n",
    "D. $P(\\text{sat}|\\text{on}) = \\frac{3}{20}$, $P(\\text{on}|\\text{sat}) = \\frac{1}{20}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5256e6",
   "metadata": {},
   "source": [
    "**Your Answer**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a02554",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "In a Transformer’s self-attention layer, which one of the following statements is correct?\n",
    "\n",
    "A. The self-attention mechanism ensures that each token attends only to previous tokens, which allows the model to preserve causal order.\n",
    "\n",
    "B. The attention weights are computed by applying a softmax over the Query matrix, normalizing the importance of each input dimension within a single token representation.\n",
    "\n",
    "C. Increasing the dimension of the Query and Key vectors does not change the computational complexity of the attention matrix.\n",
    "\n",
    "D. The scaled dot-product attention divides the dot product of Query and Key by $\\sqrt{d_k}$ to prevent the softmax function from entering regions with extremely small gradients when the vector dimension is large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbcc0f",
   "metadata": {},
   "source": [
    "**Your Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68042349",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "**In an BART/T5 model, what is the keys and queries for the cross-attention module in the decoder?**\n",
    "\n",
    "\n",
    "A. Decoder’s token embeddings as keys and encoder outputs as queries.  \n",
    "\n",
    "C. Decoder hidden states as as keys and output of the final encoder layer as queries.  \n",
    "\n",
    "D. Encoder hidden states as keys and decoder hidden states as queries.  \n",
    "\n",
    "D. The output of the final encoder layer as keys and decoder hidden states as queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdca9be7",
   "metadata": {},
   "source": [
    "**Your Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033046c8",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "**The following statements about model applicability are correct:**\n",
    "\n",
    "A. T5 can only perform generation tasks without any modification and cannot perform text classification tasks.\n",
    "\n",
    "B. BERT cannot perform text generation tasks without any modification.\n",
    "\n",
    "C. A fixed-window language model using only average pooling can perform generation tasks.\n",
    "\n",
    "D. ELMo can perform both generation and classification tasks without any modification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eb80d0",
   "metadata": {},
   "source": [
    "**Your Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f819cc0",
   "metadata": {
    "id": "1f819cc0"
   },
   "source": [
    "# Part2： Coding (70 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73903a7",
   "metadata": {
    "id": "a73903a7"
   },
   "source": [
    "\n",
    "\n",
    "In this part of the exam, you will work with a **real-world dataset** about recipe data.  \n",
    "A *recipe* is a set of instructions describing how to prepare food.  \n",
    "We will use **RecetasDeLaAbuela**, a well-known Spanish-language recipe collection gathered from the internet and released as an open-source dataset.\n",
    "\n",
    "- **Dataset size:** 20,236 rows (recipes)  \n",
    "- **Number of features:** 14 columns  \n",
    "- **Source:** [RecetasDeLaAbuela on Hugging Face](https://huggingface.co/datasets/somosnlp/RecetasDeLaAbuela)\n",
    "\n",
    "---\n",
    "\n",
    "In this part, you will use the dataset to tackle several **real-world natural language processing (NLP) tasks**, including:\n",
    "\n",
    "- **Information Retrieval**  \n",
    "- **Text Classification**  \n",
    "- **Text Generation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xdtG1nuiThug",
   "metadata": {
    "id": "xdtG1nuiThug"
   },
   "source": [
    "### Schema for *RecetasDeLaAbuela* Dataset\n",
    "\n",
    "- **Id**: `int`  \n",
    "  Unique identifier of the recipe.  \n",
    "\n",
    "- **Nombre**: `string`  \n",
    "  Name of the recipe.  \n",
    "\n",
    "- **URL**: `string`  \n",
    "  Source web link of the recipe.  \n",
    "\n",
    "- **Ingredientes**: `list[string]`  \n",
    "  List of ingredients required for the recipe.  \n",
    "\n",
    "- **Pasos**: `list[string]`  \n",
    "  Step-by-step cooking instructions.  \n",
    "\n",
    "- **Pais**: `string`  \n",
    "  Country of origin (ISO-3 code, e.g., \"MEX\").  \n",
    "\n",
    "- **Duracion**: `string`  \n",
    "  Total cooking time in hours and minutes.  \n",
    "\n",
    "- **Categoria**: `string`  \n",
    "  Category of the recipe (e.g., \"tacos\").  \n",
    "\n",
    "- **Contexto**: `string`  \n",
    "  Additional context or background information.  \n",
    "\n",
    "- **Valoracion y Votos**: `string`  \n",
    "  Ratings and votes from users (may include both numeric and text).  \n",
    "\n",
    "- **Comensales**: `int`  \n",
    "  Number of servings.  \n",
    "\n",
    "- **Tiempo**: `string`  \n",
    "  Type of dish (e.g., *Postre* = Dessert).  \n",
    "\n",
    "- **Dificultad**: `string`  \n",
    "  Difficulty level (e.g., \"easy\", \"medium\", \"hard\").  \n",
    "\n",
    "- **Valor nutricional**: `string`  \n",
    "  Nutritional value or notes (e.g., \"high in protein\").  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eba1c8b",
   "metadata": {},
   "source": [
    "You may load these libraries — they might be useful for your exam.\n",
    "You can also load any other libraries you find necessary.\n",
    "It may take some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7450328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd6ba18",
   "metadata": {
    "id": "5dd6ba18"
   },
   "source": [
    "You can use `datasets` to load this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f1d66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217,
     "referenced_widgets": [
      "369106453ccd494fad99f6ee7203d475",
      "a4efe0a5acbe4bebbf7d557811c008e1",
      "080d35728d7f48dfab0065e67880324a",
      "df2078a9236044ffa12df61f741ee788",
      "fd254a0eaa114970ba807b4c48e37199",
      "db15b7a4940042699193a61aefcbab83",
      "c707e28ad8a941f6bb26eabc8931de92",
      "ed3ac788609044a09880c27eda81fee7",
      "4fb50492996849ed85d6f011daee2d04",
      "47af5beff8d847cc90649a4b4c7c8fd7",
      "9487f8f5d423499284f93c44706bf3cc",
      "2bbbf4c74cd141bab3fe59d13d6d6cc4",
      "f3b50e3b6071491c83dbfa0eea48cadf",
      "cf3913941d304f27915e7606d9090e26",
      "80ced342c8534be4b20388e3f5d3dd1d",
      "734d36b7e13649719ea55b151fb7d8d4",
      "0cccdb150adf4c73aa3135afb687f0c6",
      "0b91681715684c039eec70fa872b3084",
      "dd193a37617947649ea6a81ecc241021",
      "d17ae2057c9e4384a9773605d8dce3f5",
      "b014d6c195e04f6b8a8db82a2debb571",
      "33cf7081dd4e4d0d8582ea06ba4850c8",
      "2d021f2935964ff68671090f185e4a32",
      "06e656363c984b3c8dec050998508e22",
      "b8a25893d8744bdb87be3b5527ed4422",
      "ec583a9f34cd418aab609f526dd2284b",
      "309bae1703e842909de105f2f8ed8d79",
      "98ed92a18544465e93f3d6a3b94d25c0",
      "0daca460254e4b83b5a7745c21a913d2",
      "a839a5f9b5e24cca8752f0711da69b4a",
      "4857268d0a014cc59dca25902a2732b5",
      "536f58154a624908895079124036e741",
      "1e01f5b59ece470dae4d552a0e2f998a"
     ]
    },
    "id": "078f1d66",
    "outputId": "13f46c9a-38fe-400e-91a6-bea741fd7585"
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"somosnlp/RecetasDeLaAbuela\", \"version_1\")[\"train\"]\n",
    "\n",
    "print(\"Number of rows:\", ds.num_rows)\n",
    "print(\"Number of features:\", len(ds.column_names))\n",
    "\n",
    "# Convert to pandas DataFrame for easier manipulation, if needed\n",
    "df = pd.DataFrame(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c51a01",
   "metadata": {},
   "source": [
    "In the following section, you will complete Parts 2.1–2.4. These parts are **independent**, and you may complete them in any order you prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gI8O8SpaUkNQ",
   "metadata": {
    "id": "gI8O8SpaUkNQ"
   },
   "source": [
    "## Part 2.1 Data Preprocessing (15 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19cc56b",
   "metadata": {},
   "source": [
    "In this section, we will focus on preparing the dataset for further analysis and modeling. \n",
    "The tasks include exploring the distribution of specific features, analyzing features, handling missing features, and creating new features. These preprocessing steps are essential for ensuring the dataset is clean, consistent, and ready for downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b210d",
   "metadata": {},
   "source": [
    "**[2 pt] 1.** Find recipes where `Categoria` is **\"vegetarianos\"**. Report the number of such recipes **per country** (`Pais`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f252c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "117974ad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jHmMlwB8Yrad",
   "metadata": {
    "id": "jHmMlwB8Yrad"
   },
   "source": [
    "**[4 pt] 2.** The dataset records the cooking duration of each dish in the column `Duracion`, though the time values are not always consistent across entries. If a time is written in the format 12:00, it means 12 minutes; if it is written as 1:00:00, it means one hour.\n",
    "\n",
    "**Task:** \n",
    "1) report the **maximum cooking time** observed in `Duracion`, and  \n",
    "2) report the quantile that mark the **bottom 20%** and **top 20%** of cooking times (sorted ascending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ph3ZoqRKZnPW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ph3ZoqRKZnPW",
    "outputId": "134fa3d0-2579-4948-f95b-9981f4ffb6de"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28e986bb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x-9FXBBMXr3F",
   "metadata": {
    "id": "x-9FXBBMXr3F"
   },
   "source": [
    "**/Discussion:/ [3 pt] 3.** Nearchos find there are differences in measurement units (e.g., `\"1 cup sugar\"` vs. `\"200 g sugar\"`) in the Recipe dataset.\n",
    " \n",
    "Give one example of a machine learning application where such inconsistencies in units could negatively affect model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a845ea9",
   "metadata": {
    "id": "6a845ea9"
   },
   "source": [
    "**Your Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f388d6e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mvt3lNCuWyGR",
   "metadata": {
    "id": "Mvt3lNCuWyGR"
   },
   "source": [
    "**[3 pt] 4.** Calculates the missing value ratio per feature and plots a sorted bar chart with features on the x-axis (sorted) and percentage on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jUGoTNukXXVx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "id": "jUGoTNukXXVx",
    "outputId": "96911167-8198-411f-ef43-10c4318114aa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d601d57",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb824d1",
   "metadata": {
    "id": "4cb824d1"
   },
   "source": [
    "**[3 pt] 5.** Create a new feature *\"Ingredientes_mean_by_categoria\"* for the dataset. This feature should represent the average number of ingredients for all recipes within the same \"Categoria\" as a given recipe. \n",
    "\n",
    "The column \"Ingredientes\" contains a string of ingredients separated by commas.\n",
    "\n",
    "Output the value of the newly created \"Ingredientes_mean_by_categoria\" feature for the recipe with Id = 1984."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QafUZKUIPjmx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QafUZKUIPjmx",
    "outputId": "9f9e26da-e8e6-4dd6-f623-d7d31b53767f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c1e91a1",
   "metadata": {
    "id": "6c1e91a1"
   },
   "source": [
    "## Part 2.2 Information Retrieval (22 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd372e9e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this section, we will focus on retrieving relevant recipes based on textual queries. The tasks include creating a TF-IDF matrix, performing recipe retrieval using cosine similarity, and analyzing the retrieval results. These steps are essential for building a robust information retrieval system for the recipe dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d160c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sxFQXbnKF16D",
   "metadata": {
    "id": "sxFQXbnKF16D"
   },
   "source": [
    "**[4 pt] 6.** Create a TF-IDF matrix using the `TfidfVectorizer` from sklearn, setting `max_features=500`.\n",
    "\n",
    "Use both the `Nombre` and `Ingredientes` columns of the recipes as the input text for the vectorizer. Combining the `Nombre` and `Ingredientes` columns into a single text column separated by a space.\n",
    "\n",
    "Print the resulting TF-IDF matrix shape.\n",
    "\n",
    "**/Discussion:/**  Explain the meaning of each dimension in the matrix shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HOcRZ4_YUps8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOcRZ4_YUps8",
    "outputId": "574d870a-7e8f-4a40-8ee7-eb099ad0d915"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "zsKWBCqzV8tF",
   "metadata": {
    "id": "zsKWBCqzV8tF"
   },
   "source": [
    "\n",
    "**Your Answer**:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05796d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zulpvXffyehD",
   "metadata": {
    "id": "zulpvXffyehD"
   },
   "source": [
    "**[4 pt] 7.** Nearchos is a fan of Greek cuisine, and he wants to search for a Greek salad (query = \"Ensalada griega\"). Here, Ensalada means salad, and griega means Greek. Help him use the same TfidfVectorizer as above to perform the retrieval, and **output the top 5 most relevant recipes with their ID, name, and cosine similarity score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B1HyLH6fWnep",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1HyLH6fWnep",
    "outputId": "f03b8f7c-799d-4988-9266-6bed672c4cbf"
   },
   "outputs": [],
   "source": [
    "query = \"ensalada griega\"\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5d3ee1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A5veiI8y0ASD",
   "metadata": {
    "id": "A5veiI8y0ASD"
   },
   "source": [
    "**[6 pt] 8.** Nearchos is disappointed with these retrieval results in Question 7 because he feels they do not reflect the Greek style he had in mind. we found that the dataset contains an item named `ensalada de berenjenas griega` that should but did not appear among the results above.\n",
    " \n",
    " Can you check whether there is a problem with this TF-IDF retrieval system? \n",
    " \n",
    " If you think there is no problem, explain why Nearchos’s desired results are missing. \n",
    " \n",
    " If there is a problem, identify what it is and re-implement the corrected code here, then output the top 5 most relevant recipes with their ID, name, and similarity score based on the corrected system. (Your corrected system must also use TF-IDF for retrieval.)\n",
    "\n",
    " You may need both code and **/Discussion:/** here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab1a225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0209c49",
   "metadata": {},
   "source": [
    "**Your Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c3375",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nEgzrryYBo7-",
   "metadata": {
    "id": "nEgzrryYBo7-"
   },
   "source": [
    "**[4 pt] 9.** **/Discussion:/**  In recipe retrieval, some recipes are longer than others. How might this length variation potentially affect TF-IDF, and what would be a simple way to test for this potential bias? Please explain in words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedc6a7a",
   "metadata": {},
   "source": [
    "**Your Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56470aff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yKkX7IUkGPlb",
   "metadata": {
    "id": "yKkX7IUkGPlb"
   },
   "source": [
    "**[4 pt] 10.** **/Discussion:/** In retrieval, polysemy may occur: for example, *pepper* can refer to *black pepper* or *bell pepper*, which are actually different. Do you think TF-IDF and word embeddings based on Word2Vec are expected to perform well in this case? Why or why not? What simple ideas do you have to address this problem? Please explain in words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e36f4",
   "metadata": {},
   "source": [
    "**Your Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d5f4e5",
   "metadata": {
    "id": "d6d5f4e5"
   },
   "source": [
    "## Part 2.3 Recipe Classification (25 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a181b5a8",
   "metadata": {},
   "source": [
    "In this section, we will focus on building a classification model to predict the country of origin (`Pais`) for each recipe based on its textual features. The tasks include data preparation, tokenization, and training a BERT-based model for sequence classification. We will also evaluate the model's performance using metrics such as F1-score and analyze the results to identify areas for improvement. This section aims to demonstrate the application of modern NLP techniques to a real-world text classification problem.\n",
    "\n",
    "In this part, you will use a **BERT model trained on Spanish** to classify recipes and predict **which country (`País`)** a dish belongs to.\n",
    "\n",
    "- **Model:** [`dccuchile/bert-base-spanish-wwm-cased`](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased)  \n",
    "- **Purpose:** Predict the country of origin for each recipe based on its text.  \n",
    "- **Note:** The internal details of the model are not crucial for this task,  \n",
    "  but you can explore them at the link above to better understand its architecture and pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a4da00",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qzQCKHPXSLrG",
   "metadata": {
    "id": "qzQCKHPXSLrG"
   },
   "source": [
    "**[3 pt] 11.** Create a new dataset that only contains the four columns `Nombre`, `Ingredientes`, `Pasos`, and `Pais` from the original dataset.  \n",
    "\n",
    "Drop all rows where `Pais` is missing, as well as all other columns.  \n",
    "\n",
    "Use `sklearn.model_selection.train_test_split` to split the dataset into `70% train` and `30% test`, with the random seed set to `42` (using random_state=42 in train_test_split function).  \n",
    "\n",
    "Output the number of samples contained in the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PRTvfg5uSKvg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRTvfg5uSKvg",
    "outputId": "20d53d67-1fe3-471c-ebb6-155a56b73484"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dc9e2a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AS5ORmoxFENc",
   "metadata": {
    "id": "AS5ORmoxFENc"
   },
   "source": [
    "**[2 pt] 12.** For the training and test datasets, prepare them in the Hugging Face dataset format. Specifically:\n",
    "\n",
    "- Add a new column `\"labels\"`, where the values are in a format that can be directly used by a classification model.  \n",
    "- Add a new column `\"text\"`, which should be created by combining the original `\"Nombre\"` and `\"Ingredientes\"` columns, separated by a space.\n",
    "- Finally, print a sample from the test dataset to confirm the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wxChNrURFEnn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170,
     "referenced_widgets": [
      "1b0d13e9a7f34497888e378bab058f1d",
      "e5b61f8e04fd4c998c705ee353660ceb",
      "2116347db84a4d8da3f241ccc3b21343",
      "ab7d8f95d93843088aeba3a5bde21a95",
      "ac16165e46fe4898bd5112930c8291a7",
      "4e4e7e31d8e440689279d8438bfc035f",
      "5b1e5cf6cf6640119aebb6f337756c23",
      "7a8a4f4bb38640bf8ac403162e91031d",
      "c57069e542c04d47bc2f8b423b107f56",
      "145d8ac27b4f489f9dcf5b1422bd0e7b",
      "c06c3ca81d624881a084afcecbc1ed49",
      "ee3ba2777d0a4dec9c1f231bbe983c18",
      "55255b77f15947b296be425a0ef55b10",
      "570710a944664d64bd4aca91a97e59c0",
      "626c4f2c6ae64a5283c9a84d35b0a5c9",
      "c14c295c2b314bf4abd59ab89c7ffe71",
      "21da6ada0a2a424483026deb9d961994",
      "37e4026513374e72a1370efd98ebf032",
      "f0140a0e667149968c68818a01afac65",
      "7c1192aac177402490f754abd3e62301",
      "c65a00808ee147adb4c0cf969d47a05f",
      "d70ebbdf353a49d48a994d124aec52cc"
     ]
    },
    "id": "wxChNrURFEnn",
    "outputId": "978a7c37-3d96-4334-e85e-cc552c786575"
   },
   "outputs": [],
   "source": [
    "labels = sorted(df[\"Pais\"].unique())\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTest sample:\")\n",
    "print(\"Text:\", test_dataset[0][\"text\"])\n",
    "print(\"Label:\", test_dataset[0][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1f6d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U1DaIKkQkExl",
   "metadata": {
    "id": "U1DaIKkQkExl"
   },
   "source": [
    "**[2pt] 13.** You are given a pretrained `tokenizer` and a `tokenize` function. Your task is to:\n",
    "\n",
    "1. Use the provided `tokenizer` to tokenize the `\"text\"` for both the training and test datasets.  \n",
    "2. Format the datasets so that only the columns required by the model (`input_ids`, `attention_mask`, and `labels`) are kept, in PyTorch tensor format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uoGzKBXmkF7t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "59e563367fc743559196f2a8cc8d79aa",
      "da5a920787a34176beac63a0c5c48f98",
      "ffc49bbb3f734b0b9376a313ce6bcbc9",
      "45e72ea31fc04542bd17d85d3c63748d",
      "cbae0bd233544e5fb72bf0ed36a839a3",
      "0c7fd6404c304b27a36db04d53f66953",
      "b3e6eac2d5ab46ed9cc63f19229849ba",
      "33ccc3be4e604c898f41eff96f62ea9e",
      "7d8e53cc598b4f44905895ebba0d7185",
      "6d0c0cc08f214b299154568961d9ba82",
      "a6f30714e6aa483b902ca1fb9984172d",
      "66a58c0bf6594caaa3e03cdd99b45044",
      "450bd2f186024986a5cceece2b6c8bad",
      "0d74eaab93044f6c8208757ef48b3e3b",
      "0ca30687769f4328948f60c0d9c65c93",
      "d715d2e4c2b24d12b36b5ec0eae56e1e",
      "19bfcda18fe14b0f99ca4f3d382c90fa",
      "46868da479c849d6a7c21867908a2333",
      "bd56608ea6fb4fb8b2b0e9d596cfc198",
      "e7c37e4d2b6a407cb271a784d32b8074",
      "07ace6dff56542ffbb2c3134ef0a7118",
      "a81d3980bf3b45e5b09b2b6a914222d2",
      "6d3771475fc44ea2b7364a19a24e3f98",
      "6ceb7cc4ba6a430d9acb38d3577d15dd",
      "ffb1f0c2b9d5451496fd76426dd79426",
      "a05500b92c414ba38ee0597f5f36c1c1",
      "828aa23af2a64c19b0db3b861cc93a0b",
      "2576ea166afc474dacfc1de0cb9a2519",
      "49cae0bc742c44c395b7b171187d87f6",
      "0b24692811c345c8a60fb29bf9de49de",
      "4c930a55b9ee47eda18a7dd8f7490439",
      "aadfb560d1fa463092fcce0569164c71",
      "8e788847fe6549f89a23f75a1106a300",
      "91c4d1827f704fa3a4b582fa0528de7b",
      "a2101960500840a1afd2cb2ab78dcbf8",
      "b7aa4b5caa414ba28d659a3f4bde1f96",
      "f83e4fb0b4584da3891487cc0b054072",
      "db2bf84a654f4d519e48c510f03b9a76",
      "79028b91acc34b16b9abed5ff01ddfbe",
      "4d6c323431724286af33ecb85bfbb006",
      "1d398ac55a0a442ba3a1a939975fc17f",
      "10d0388828b646f69116e6ed19012138",
      "b0bceddb2c9a47289e22e2aa418e8cf6",
      "23fc0aa40f174aacbd70108e49ce62ee",
      "aefd8811fd6646ae8091fc42340e3b2d",
      "ccfeb3741e5e4a9fa61f8bd62d53b8a0",
      "a2e088fc95584b97af57991fdcc82777",
      "1519704ee607406d958809607af7f3a7",
      "14f02f3aacaf475a9ab7a0a062a2f7ac",
      "57ac73c324cd44bf91d942659879f06b",
      "6068d8fba19b43cab7aaa82dd028690a",
      "012dacbcd96b49a58d348e094746a430",
      "17126b881c5748fe9f53c12d3619cb8d",
      "09db43355fa542d08b54d029f64b8354",
      "c4f6a90a03b34bdbbe1e3540357277fe"
     ]
    },
    "id": "uoGzKBXmkF7t",
    "outputId": "458ecce1-9a7a-4a70-fdb2-a415334e8c2f"
   },
   "outputs": [],
   "source": [
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "def tokenize(batch):\n",
    "    \"\"\"\n",
    "    Tokenize the input batch of text examples.\n",
    "\n",
    "    Args:\n",
    "        batch (dict): A dictionary containing a key \"text\", \n",
    "                      where each value is a string to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing tokenized representations that can be directly used for model training or evaluation.\n",
    "    \"\"\"\n",
    "        \n",
    "    #YOUR CODE HERE\n",
    "\n",
    "\n",
    "#YOUR CODE HERE\n",
    "\n",
    "sample = train_dataset[0]\n",
    "print(\"Sample input_ids length:\", len(sample[\"input_ids\"]))\n",
    "print(\"Sample label:\", sample[\"labels\"].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4dc22",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jb1l4zWimmrd",
   "metadata": {
    "id": "jb1l4zWimmrd"
   },
   "source": [
    "**[3 pt] 14.** Define a function `compute_metrics(eval_pred)` that will be used in Hugging Face’s `Trainer` to evaluate the performance of a classification model.\n",
    "\n",
    "- `Input:`  \n",
    "  - `eval_pred`: a tuple `(logits, labels)`  \n",
    "    - `logits`: the raw model outputs as a NumPy array of shape `(batch_size, num_labels)`  \n",
    "    - `labels`: the ground-truth label IDs as a NumPy array of shape `(batch_size,)`  \n",
    "\n",
    "- `Processing:`  \n",
    "\n",
    "  - Compute the `micro F1-score` and `macro F1-score` using the predicted labels and the ground-truth labels.  \n",
    "\n",
    "- `Output:`  \n",
    "  - A dictionary with the following keys and values:  \n",
    "    ```python\n",
    "    {\n",
    "        \"micro_f1\": <micro F1 score>,\n",
    "        \"macro_f1\": <macro F1 score>\n",
    "    }\n",
    "    ```  \n",
    "**You should run the code and display the output corresponding to our input.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FSr0t5cJcI23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSr0t5cJcI23",
    "outputId": "1ec90362-8391-41a5-f0da-e9b186bb3794"
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred: tuple) -> dict:\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics (micro and macro F1 scores) for model predictions.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple): A tuple (logits, labels)\n",
    "            - logits (np.ndarray): Model output scores of shape (num_samples, num_classes)\n",
    "            - labels (np.ndarray): True class labels of shape (num_samples,)\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys \"micro_f1\" and \"macro_f1\" containing\n",
    "              the respective F1 scores.\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    " \n",
    "\n",
    "    return {\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"macro_f1\": macro_f1\n",
    "    }\n",
    "\n",
    "logits = np.array([\n",
    "    [2.1, 0.3, 0.1],\n",
    "    [0.2, 1.5, 0.4],\n",
    "    [0.1, 0.4, 2.0],\n",
    "    [1.0, 1.2, 0.5],\n",
    "    [0.3, 0.2, 2.5]\n",
    "])\n",
    "\n",
    "\n",
    "labels = np.array([0, 1, 2, 0, 2])\n",
    "results = compute_metrics((logits, labels))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a6cebb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wh0wTVwjo0q9",
   "metadata": {
    "id": "Wh0wTVwjo0q9"
   },
   "source": [
    "**[2 pt] 15.** Define a Hugging Face `Trainer` for our classification task. You must carefully configure it to include all the necessary parameters (for example, the dataset defined in the previous questions). \n",
    "\n",
    "For the training parameters (`training_args`), you need to keep them exactly the same as the ones we provided.\n",
    "\n",
    "\n",
    "Due to resource limitations, you are not required to run the training process; You need to run `trainer.train()` and print the loss values for the first 100 steps to ensure that your code runs correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79L2ZxnSm8jz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 983,
     "referenced_widgets": [
      "23f8c59fe309405aaac7f4fd903ab21f",
      "ec49468101bf4a399bedeabfb740eddf",
      "5e9820be00824005978875ca65b36d19",
      "f12fc665bfdc4285b2ce0c6b02c81ae0",
      "2130c2f5bc244e5ea9c62512de4dafd1",
      "70c107e511a34f82a1f16ae0891ce1e1",
      "5d01c7dcaff44d048bed48f2b35122de",
      "07bcb89fd7394cbf83aa81917a22d06f",
      "699d3d2a153d42379d527efc51006041",
      "71cc553e83f44da1adb3ed8b35d12a11",
      "83f21d4234ef4910a8af452bd3e7e1bf"
     ]
    },
    "id": "79L2ZxnSm8jz",
    "outputId": "19aad8b0-afdf-487b-a70e-717c3729035e"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    save_strategy=\"no\",                 \n",
    "    max_steps=100,              \n",
    "    logging_steps=10,          \n",
    "    learning_rate=2e-6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    #YOUR CODE HERE\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f96a663",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m675p50LJ_x3",
   "metadata": {
    "id": "m675p50LJ_x3"
   },
   "source": [
    "**Nearchos has already trained a model for you. You can load it and check its results on the test dataset. Next, we will try to evaluate the results and discuss a series of related questions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jMI4bn3CJMbr",
   "metadata": {
    "id": "jMI4bn3CJMbr"
   },
   "outputs": [],
   "source": [
    "df_loaded = pd.read_csv(\"test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2dd118",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dYPgLagQKPim",
   "metadata": {
    "id": "dYPgLagQKPim"
   },
   "source": [
    "**[3 pt] 16.** Generate and **visualize** a `confusion matrix` that compares the true and predicted labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Oon9OokNKs22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "id": "Oon9OokNKs22",
    "outputId": "dc1a58bc-703c-4215-dc78-6f11b79f7545"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f650b73e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PP2HPSPzFjJh",
   "metadata": {
    "id": "PP2HPSPzFjJh"
   },
   "source": [
    "**[3 pt] 17.** **/Discussion:/** For `Macro-F1` and `Micro-F1`, which metric would you choose as the main training objective? Justify **Your Answer**:.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iQqGIBD9NjC8",
   "metadata": {
    "id": "iQqGIBD9NjC8"
   },
   "source": [
    "**Your Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5b0381",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oaml5c4LM5rN",
   "metadata": {
    "id": "oaml5c4LM5rN"
   },
   "source": [
    "**[3 pt] 18.** **/Discussion:/**  Nearchos was very surprised to find that the model performed very well in predicting Uruguayan (URY) dishes. He therefore concluded that the classifier has a very strong ability to distinguish Uruguayan cuisine. Do you think he is correct? Explain your point of view. You can use code to display something that supports your response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ccd026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Gr0-3aL8NNTi",
   "metadata": {
    "id": "Gr0-3aL8NNTi"
   },
   "source": [
    "**Your Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8176cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hDAZ-BmgN29V",
   "metadata": {
    "id": "hDAZ-BmgN29V"
   },
   "source": [
    "**[4 pt] 19.** **/Discussion:/** Nearchos observed that the model performs poorly on some classes with very few samples. Can you propose **TWO** methods to help Nearchos improve the performance on these low-resource classes? And explain why you think these methods can enhance the model’s performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T-iysgPYN2sJ",
   "metadata": {
    "id": "T-iysgPYN2sJ"
   },
   "source": [
    "**Your Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f6cbb6",
   "metadata": {
    "id": "c5f6cbb6"
   },
   "source": [
    "## Part 2.4 Text Generation - Recipe Title Generation (8 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e54c7",
   "metadata": {},
   "source": [
    "In this section, we will focus evaluating the diversity of generated titles and discussing the limitations of BLEU-based methods for diversity evaluation. This section aims to explore the challenges and opportunities in applying text generation techniques to recipe datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NeEhJBxTEY0R",
   "metadata": {
    "id": "NeEhJBxTEY0R"
   },
   "source": [
    "Nearchos has recently been using LLMs to generate recipes. As a crazy recipe inventor, he needs the generated recipes to be diverse, which means each time the LLM runs, it should produce different recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytSq6zhFz5l",
   "metadata": {
    "id": "pytSq6zhFz5l"
   },
   "source": [
    "BLEU (https://en.wikipedia.org/wiki/BLEU) is an automatic evaluation metric that measures the similarity between a generated text and reference texts. You can learn more about BLEU through online resources or by consulting an LLM assistant.\n",
    "Next, we will take a step further and adapt BLEU to measure diversity.\n",
    "\n",
    "Here is an example of BLEU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ob1yKjR-Fx8s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ob1yKjR-Fx8s",
    "outputId": "412d1c11-a822-447c-fef3-94965eca4f2b"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "reference = [\n",
    "    [\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"],\n",
    "    [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]\n",
    "]\n",
    "\n",
    "candidate = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "score = sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "print(\"BLEU score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a12dd36",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UxX3HqoNGj4n",
   "metadata": {
    "id": "UxX3HqoNGj4n"
   },
   "source": [
    "**[4 pt] 20.** Using the BLEU function provided above and the following example `titles` (group1 - group4), implement a method to calculate diversity BLEU: For each sentence, we treat it as the candidate and use all other sentences as references to compute a BLEU score. Finally, we take the average BLEU score across all sentences. \n",
    "You should use the same smoothing_function as in the example above. Make sure your program prints out the final score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c8415e",
   "metadata": {},
   "source": [
    "Here are the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FuwHkWsNLa8M",
   "metadata": {
    "id": "FuwHkWsNLa8M"
   },
   "outputs": [],
   "source": [
    "group1 = [\n",
    "    \"Spaghetti with homemade tomato sauce\",\n",
    "    \"Pasta with fresh tomato sauce\",\n",
    "    \"Spaghetti in classic marinara sauce\"\n",
    "]\n",
    "\n",
    "group2 = [\n",
    "    \"Rich chocolate layer cake with frosting\",\n",
    "    \"Grilled salmon with lemon butter\",\n",
    "    \"Spicy vegetable curry with coconut milk\"\n",
    "]\n",
    "\n",
    "group3 = [\n",
    "    \"Crispy French fries with sea salt\",\n",
    "    \"Golden fried potato sticks\",\n",
    "    \"British-style chips served with vinegar\"\n",
    "]\n",
    "\n",
    "group4 = [\n",
    "    \"Classic American hot dog with mustard\",\n",
    "    \"Traditional dog stew with vegetables\",\n",
    "    \"Spicy chili dog with melted cheese\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fC1Wo1vIJ62r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fC1Wo1vIJ62r",
    "outputId": "b6784a5c-9469-48e2-9eca-2cc2ead76db8"
   },
   "outputs": [],
   "source": [
    "def self_bleu(sentences):\n",
    "    \"\"\"\n",
    "    Compute the Self-BLEU score for a list of generated sentences.\n",
    "\n",
    "    The Self-BLEU metric measures the degree of similarity among generated sentences.\n",
    "    A higher Self-BLEU score indicates lower diversity (sentences are more similar),\n",
    "    while a lower Self-BLEU score indicates higher diversity.\n",
    "\n",
    "    Args:\n",
    "        sentences (list[str]): A list of sentences (strings) generated by the model.\n",
    "\n",
    "    Returns:\n",
    "        float: The average Self-BLEU score across all sentences.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "\n",
    "\n",
    "print(\"Group1 Self-BLEU:\", self_bleu(group1))\n",
    "print(\"Group2 Self-BLEU:\", self_bleu(group2))\n",
    "print(\"Group3 Self-BLEU:\", self_bleu(group3))\n",
    "print(\"Group4 Self-BLEU:\", self_bleu(group4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9565f174",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VYC-oEA7Gjtm",
   "metadata": {
    "id": "VYC-oEA7Gjtm"
   },
   "source": [
    "**[4 pt] 21.** **/Discussion:/** Based on the previous examples, discuss the potential drawbacks of applying this BLEU-based method to evaluate the diversity of recipe titles. What kind of evaluation method do you think could improve this, and explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qobl62ieHekX",
   "metadata": {
    "id": "Qobl62ieHekX"
   },
   "source": [
    "**Your Answer**:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp25-exam",
   "language": "python",
   "name": "exam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
